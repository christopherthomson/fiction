{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running multiple train-test-validation splits\n",
    "\n",
    "### to better estimate predictive accuracy in modeling genre.\n",
    "\n",
    "This notebook attempts a slight improvement on the methods deployed in my 2015 article, \"The Life Cycles of Genres.\"\n",
    "\n",
    "In 2015, I used a set number of features and a set regularization constant. Now I optimize *n* (number of features) and *c* (the regularization constant) through gridsearch, running multiple crossvalidations on a train/test set to find the best constants for a given sample.\n",
    "\n",
    "To avoid exaggerating accuracy through multiple trials, I have also moved to a train/test/validation split: constants are optimized through crossvalidation on the train-test set, but the model is then tested on a separate validation set. I repeat that process on random train/test/validation splits in order to visualize model accuracy as a distribution.\n",
    "\n",
    "Getting the train/test vs. validation split right can be challenging, because we want to avoid repeating *authors* from the train/test set in validation. (Or in both train and test for that matter.) Authorial diction is constant enough that this could become an unfair advantage for genres with a few prolific authors. We also want to ensure that the positive & negative classes within a given set have a similar distribution across historical time. (Otherwise the model will become a model of language change.) Building sets where all these conditions hold is more involved than a random sample of volumes.\n",
    "\n",
    "Most of the code in this notebook is concerned with creating the train/test-vs-validation split. The actual modeling happens in versatiletrainer2, which we import in the first cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os, csv, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import versatiletrainer2\n",
    "import metaselector\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Managing the validation split.\n",
    "\n",
    "The functions defined below are used to create a train/test/validation divide, while also ensuring\n",
    "\n",
    "1. No author is present in more than one of those sets, so we don't overfit on a specific style.\n",
    "2. Positive and negative classes are equally distributed across time (so we don't end up modeling language change instead of genre!)\n",
    "\n",
    "But the best way to understand the overall workflow may be to scan down a few cells to the bottom function, **train_and_validate().**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evenlymatchdate(meta, tt_positives, v_positives, negatives):\n",
    "    '''\n",
    "    Given a metadata file, two lists of positive indexes and a (larger) list\n",
    "    of negative indexes, this assigns negatives that match the date distribution\n",
    "    of the two positive lists as closely as possible, working randomly so that\n",
    "    neither list gets \"a first shot\" at maximally close matches.\n",
    "    \n",
    "    The task is complicated by our goal of ensuring that authors are only\n",
    "    represented in the train/test OR the validation set. To do this while\n",
    "    using as much of our sample as we can, we encourage the algorithm to choose\n",
    "    works from already-selected authors when they fit the date parameters needed.\n",
    "    This is the function of the selected_neg_unmatched set: works by authors we have\n",
    "    chosen, not yet matched to a positive work.\n",
    "    '''\n",
    "    \n",
    "    assert len(negatives) > (len(tt_positives) + len(v_positives))\n",
    "    authors = dict()\n",
    "    authors['tt'] = set(meta.loc[tt_positives, 'author'])\n",
    "    authors['v'] = set(meta.loc[v_positives, 'author'])\n",
    "    \n",
    "    neg_matched = dict()\n",
    "    neg_matched['tt'] = []\n",
    "    neg_matched['v'] = []\n",
    "    neg_unmatched = dict()\n",
    "    neg_unmatched['v'] = []\n",
    "    neg_unmatched['tt'] = []\n",
    "    \n",
    "    negative_meta = meta.loc[negatives, : ]\n",
    "    \n",
    "    allpositives = [(x, 'tt') for x in tt_positives]\n",
    "    allpositives.extend([(x, 'v') for x in v_positives])\n",
    "    random.shuffle(allpositives)\n",
    "    \n",
    "    for idx, settype in allpositives:\n",
    "        if settype == 'v':\n",
    "            inversetype = 'tt'\n",
    "        else:\n",
    "            inversetype = 'v'\n",
    "            \n",
    "        date = meta.loc[idx, 'firstpub']\n",
    "        found = False\n",
    "        negative_meta = negative_meta.assign(diff = np.abs(negative_meta['firstpub'] - date))\n",
    "        \n",
    "        for idx2 in neg_unmatched[settype]:\n",
    "            matchdate = meta.loc[idx2, 'firstpub']\n",
    "            if abs(matchdate - date) < 3:\n",
    "                neg_matched[settype].append(idx2)\n",
    "                location = neg_unmatched[settype].index(idx2)\n",
    "                neg_unmatched[settype].pop(location)\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            candidates = []\n",
    "            for i in range(200):\n",
    "                aspirants = negative_meta.index[negative_meta['diff'] == i].tolist()\n",
    "                \n",
    "                # the following section insures that authors in\n",
    "                # traintest don't end up also in validation\n",
    "                for a in aspirants:\n",
    "                    asp_author = meta.loc[a, 'author']\n",
    "                    if asp_author not in authors[inversetype]:\n",
    "                        # don't even consider books by authors already\n",
    "                        # in the other set\n",
    "                        candidates.append(a)\n",
    "                        \n",
    "                if len(candidates) > 0:\n",
    "                    break\n",
    "        \n",
    "            chosen = random.sample(candidates, 1)[0]\n",
    "            chosenauth = negative_meta.loc[chosen, 'author']\n",
    "            allbyauth = negative_meta.index[negative_meta['author'] == chosenauth].tolist()\n",
    "            authors[settype].add(chosenauth)\n",
    "            \n",
    "            if len(allbyauth) < 1:\n",
    "                print('error')\n",
    "                \n",
    "            for idx3 in allbyauth:\n",
    "                if idx3 == chosen:\n",
    "                    neg_matched[settype].append(idx3)\n",
    "                    # the one we actually chose\n",
    "                else:\n",
    "                    neg_unmatched[settype].append(idx3)\n",
    "                    # others by same author, to be considered first in future\n",
    "            \n",
    "            negative_meta.drop(allbyauth, inplace = True)\n",
    "            \n",
    "            if len(negative_meta) == 0:\n",
    "                print('Exhausted negatives! This is surprising.')\n",
    "                break\n",
    "    \n",
    "    # other books by same authors can be added to the set in the end\n",
    "    tt_neg = neg_matched['tt'] + neg_unmatched['tt']\n",
    "    v_neg = neg_matched['v'] + neg_unmatched['v']\n",
    "    \n",
    "    remaining_neg = negative_meta.index.tolist()\n",
    "\n",
    "    return tt_neg, v_neg, remaining_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tags2tagset(x):\n",
    "    ''' function that will be applied to transform\n",
    "    fantasy|science-fiction into {'fantasy', 'science-fiction'} '''\n",
    "    if type(x) == float:\n",
    "        return set()\n",
    "    else:\n",
    "        return set(x.split(' | '))\n",
    "\n",
    "def divide_training_from_validation(tags4positive, tags4negative, sizecap, metadatapath):\n",
    "    ''' This function divides a dataset into two parts: a training-and-test set, and a\n",
    "    validation set. We ensure that authors are represented in one set *or* the other,\n",
    "    not both.\n",
    "    \n",
    "    A model is optimized by gridsearch and crossvalidation on the training-and-test set. Then this model\n",
    "    is applied to the validation set, and accuracy is recorded.\n",
    "    '''\n",
    "    \n",
    "    meta = pd.read_csv(metadatapath)\n",
    "    column_of_sets = meta['genretags'].apply(tags2tagset)\n",
    "    meta = meta.assign(tagset = column_of_sets)\n",
    "    \n",
    "    overlap = []\n",
    "    negatives = []\n",
    "    positives = []\n",
    "    \n",
    "    for idx, row in meta.iterrows():\n",
    "        if 'drop' in row['tagset']:\n",
    "            continue\n",
    "            # these works were dropped and will not be present in the data folder\n",
    "            \n",
    "        posintersect = len(row['tagset'] & tags4positive)\n",
    "        negintersect = len(row['tagset'] & tags4negative)\n",
    "        \n",
    "        if posintersect and negintersect:\n",
    "            overlap.append(idx)\n",
    "        elif posintersect:\n",
    "            positives.append(idx)\n",
    "        elif negintersect:\n",
    "            negatives.append(idx)\n",
    "            \n",
    "    print()\n",
    "    print('-------------')\n",
    "    print('Begin construction of validation split.')\n",
    "    print(\"Positives/negatives:\", len(positives), len(negatives))\n",
    "    \n",
    "    random.shuffle(overlap)\n",
    "    print('Overlap (assigned to pos class): ' + str(len(overlap)))\n",
    "    positives.extend(overlap)\n",
    "    \n",
    "    # We do selection by author\n",
    "    positiveauthors = list(set(meta.loc[positives, 'author'].tolist()))\n",
    "    \n",
    "    random.shuffle(positiveauthors)\n",
    "    \n",
    "    traintest_pos = []\n",
    "    validation_pos = []\n",
    "    donewithtraintest = False\n",
    "    \n",
    "    for auth in positiveauthors:\n",
    "        this_auth_indices = meta.index[meta['author'] == auth].tolist()\n",
    "        confirmed_auth_indices = []\n",
    "        for idx in this_auth_indices:\n",
    "            if idx in positives:\n",
    "                confirmed_auth_indices.append(idx)\n",
    "        \n",
    "        if not donewithtraintest:\n",
    "            traintest_pos.extend(confirmed_auth_indices)\n",
    "        else:\n",
    "            validation_pos.extend(confirmed_auth_indices)\n",
    "        \n",
    "        if len(traintest_pos) > sizecap:\n",
    "            # that's deliberately > rather than >= because we want a cushion\n",
    "            donewithtraintest = True\n",
    "    \n",
    "    # Now let's get a set of negatives that match the positives' distribution\n",
    "    # across the time axis.\n",
    "    \n",
    "    traintest_neg, validation_neg, remaining_neg = evenlymatchdate(meta, traintest_pos, validation_pos, negatives)\n",
    "    traintest = meta.loc[traintest_pos + traintest_neg, : ]\n",
    "    realclass = ([1] * len(traintest_pos)) + ([0] * len(traintest_neg))\n",
    "    traintest = traintest.assign(realclass = realclass)\n",
    "    print(\"Traintest pos/neg:\", len(traintest_pos), len(traintest_neg))\n",
    "    \n",
    "    if len(validation_neg) > len(validation_pos):\n",
    "        validation_neg = validation_neg[0: len(validation_pos)]\n",
    "        # we want the balance of pos and neg examples to be even\n",
    "        \n",
    "    print(\"Validation pos/neg:\", len(validation_pos), len(validation_neg))\n",
    "    \n",
    "    validation = meta.loc[validation_pos + validation_neg, : ]\n",
    "    realclass = ([1] * len(validation_pos)) + ([0] * len(validation_neg))\n",
    "    validation = validation.assign(realclass = realclass)\n",
    "    \n",
    "    return traintest, validation \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteratively testing multiple splits.\n",
    "\n",
    "Because we have a relatively small number of data points for our positive classes, there's a fair amount of variation in model accuracy depending on the exact sample chosen. It's therefore necessary to run the whole train/test/validation cycle multiple times to get a distribution and a median value.\n",
    "\n",
    "The best way to understand the overall workflow may be to look first at the bottom function, **train_and_validate()**.  Essentially we create a split between train/test and validation sets, and write both as temporary files. Then the first, train/test file is passed to a function that runs a grid-search on it (via crossvalidation). We get back some parameters, including cross-validated accuracy; the model and associated objects (e.g. vocabulary, scaler, etc) are pickled and written to disk.\n",
    "\n",
    "Then finally we apply the pickled model to the held-out *validation* set in order to get validation accuracy.\n",
    "\n",
    "We do all of that multiple times to get a sense of the distribution of possible outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tune_a_model(name, tags4positive, tags4negative, sizecap, sourcefolder, metadatapath):\n",
    "    '''\n",
    "    This tunes a model through gridsearch, and puts the resulting model in a ../temp\n",
    "    folder, where it can be retrieved\n",
    "    '''\n",
    "\n",
    "    vocabpath = '../lexica/' + name + '.txt'\n",
    "    modeloutpath = '../temp/' + name + '.csv'\n",
    "\n",
    "    c_range = [.0001, .001, .003, .01, .03, 0.1, 1, 10, 100, 300, 1000]\n",
    "    featurestart = 1000\n",
    "    featureend = 7000\n",
    "    featurestep = 500\n",
    "    modelparams = 'logistic', 10, featurestart, featureend, featurestep, c_range\n",
    "    forbiddenwords = {}\n",
    "    floor = 1700\n",
    "    ceiling = 2020\n",
    "\n",
    "    metadata, masterdata, classvector, classdictionary, orderedIDs, authormatches, vocablist = versatiletrainer2.get_simple_data(sourcefolder, metadatapath, vocabpath, tags4positive, tags4negative, sizecap, extension = '.fic.tsv', excludebelow = floor, excludeabove = ceiling,\n",
    "        forbid4positive = {'drop'}, forbid4negative = {'drop'}, force_even_distribution = False, forbiddenwords = forbiddenwords)\n",
    "\n",
    "    matrix, maxaccuracy, metadata, coefficientuples, features4max, best_regularization_coef = versatiletrainer2.tune_a_model(metadata, masterdata, classvector, classdictionary, orderedIDs, authormatches,\n",
    "        vocablist, tags4positive, tags4negative, modelparams, name, modeloutpath)\n",
    "\n",
    "    meandate = int(round(np.sum(metadata.firstpub) / len(metadata.firstpub)))\n",
    "    floor = np.min(metadata.firstpub)\n",
    "    ceiling = np.max(metadata.firstpub)\n",
    "\n",
    "    os.remove(vocabpath)\n",
    "    \n",
    "    return floor, ceiling, meandate, maxaccuracy, features4max, best_regularization_coef, modeloutpath\n",
    "\n",
    "def confirm_separation(df1, df2):\n",
    "    '''\n",
    "    Just some stats on the train/test vs validation split.\n",
    "    '''\n",
    "    \n",
    "    authors1 = set(df1['author'])\n",
    "    authors2 = set(df2['author'])\n",
    "    overlap = authors1.intersection(authors2)\n",
    "    if len(overlap) > 0:\n",
    "        print('Overlap: ', overlap)\n",
    "    \n",
    "    pos1date = np.mean(df1.loc[df1.realclass == 0, 'firstpub'])\n",
    "    neg1date = np.mean(df1.loc[df1.realclass == 1, 'firstpub'])\n",
    "    pos2date = np.mean(df2.loc[df2.realclass == 0, 'firstpub'])\n",
    "    neg2date = np.mean(df2.loc[df2.realclass == 1, 'firstpub'])\n",
    "    \n",
    "    print(\"Traintest mean date pos:\", pos1date, \"neg:\", neg1date)\n",
    "    print(\"Validation mean date pos\", pos2date, \"neg:\", neg2date)\n",
    "    print()\n",
    "    \n",
    "\n",
    "def train_and_validate(modelname, tags4positive, tags4negative, sizecap, sourcefolder, metadatapath):\n",
    "    \n",
    "    outmodels = modelname + '_models.tsv'\n",
    "    if not os.path.isfile(outmodels):\n",
    "        with open(outmodels, mode = 'w', encoding = 'utf-8') as f:\n",
    "            outline = 'name\\tsize\\tfloor\\tceiling\\tmeandate\\ttestacc\\tvalidationacc\\tfeatures\\tregularization\\ti\\n'\n",
    "            f.write(outline)\n",
    "    \n",
    "    for i in range(10):\n",
    "        name = modelname + str(i)\n",
    "        \n",
    "        traintest, validation = divide_training_from_validation(tags4positive, tags4negative, sizecap, metadatapath)\n",
    "        \n",
    "        confirm_separation(traintest, validation)\n",
    "    \n",
    "        traintest.to_csv('../temp/traintest.csv', index = False)\n",
    "        validation.to_csv('../temp/validation.csv', index = False)\n",
    "        \n",
    "        floor, ceiling, meandate, testacc, features4max, best_regularization_coef, modeloutpath = tune_a_model(name, tags4positive, tags4negative, sizecap, sourcefolder, '../temp/traintest.csv')\n",
    "        modelinpath = modeloutpath.replace('.csv', '.pkl')\n",
    "        results = versatiletrainer2.apply_pickled_model(modelinpath, sourcefolder, '.fic.tsv', '../temp/validation.csv')\n",
    "        right = 0\n",
    "        wrong = 0\n",
    "        columnname = 'alien_model'\n",
    "        for idx, row in results.iterrows():\n",
    "            if float(row['realclass']) >= 0.5 and row[columnname] >= 0.5:\n",
    "                right +=1\n",
    "            elif float(row['realclass']) <= 0.5 and row[columnname] <= 0.5:\n",
    "                right += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "        \n",
    "        validationacc = right / (right + wrong)\n",
    "        validoutpath = modeloutpath.replace('.csv', '.validate.csv')\n",
    "        results.to_csv(validoutpath)\n",
    "        print()\n",
    "        print('Validated: ', validationacc)\n",
    "        \n",
    "        with open(outmodels, mode = 'a', encoding = 'utf-8') as f:\n",
    "            outline = '\\t'.join([name, str(sizecap), str(floor), str(ceiling), str(meandate), str(testacc), str(validationacc), str(features4max), str(best_regularization_coef), str(i)]) + '\\n'\n",
    "            f.write(outline)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 166 370\n",
      "Overlap (assigned to pos class): 0\n",
      "Traintest pos/neg: 126 129\n",
      "Validation pos/neg: 40 40\n",
      "Traintest mean date pos: 1874.9302325581396 neg: 1873.579365079365\n",
      "Validation mean date pos 1888.6 neg: 1888.725\n",
      "\n",
      "We started with 255 rows in metadata, but\n",
      "lost 1 that were missing in the data folder.\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 125 potential positive instances and\n",
      "129 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1761 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 12, 12, 13, 12, 12, 13, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.72\n",
      "words: 1000  reg: 0.001  acc: 0.752\n",
      "words: 1000  reg: 0.003  acc: 0.772\n",
      "words: 1000  reg: 0.01  acc: 0.804\n",
      "True positives 96\n",
      "True negatives 105\n",
      "False positives 20\n",
      "False negatives 29\n",
      "F1 : 0.7966804979253111\n",
      "0.804 0.804\n",
      "80\n",
      "(80, 1000)\n",
      "\n",
      "Validated:  0.825\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 166 370\n",
      "Overlap (assigned to pos class): 0\n",
      "Traintest pos/neg: 128 132\n",
      "Validation pos/neg: 38 38\n",
      "Traintest mean date pos: 1881.0454545454545 neg: 1880.5390625\n",
      "Validation mean date pos 1864.8947368421052 neg: 1866.078947368421\n",
      "\n",
      "We started with 260 rows in metadata, but\n",
      "lost 1 that were missing in the data folder.\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 127 potential positive instances and\n",
      "132 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1760 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.704\n",
      "words: 1000  reg: 0.001  acc: 0.756\n",
      "words: 1500  reg: 0.001  acc: 0.776\n",
      "words: 2000  reg: 0.001  acc: 0.788\n",
      "words: 2000  reg: 0.003  acc: 0.792\n",
      "words: 3500  reg: 0.001  acc: 0.796\n",
      "words: 4000  reg: 0.001  acc: 0.8\n",
      "words: 4500  reg: 0.001  acc: 0.804\n",
      "words: 5000  reg: 0.001  acc: 0.808\n",
      "words: 5500  reg: 0.003  acc: 0.812\n",
      "True positives 95\n",
      "True negatives 108\n",
      "False positives 17\n",
      "False negatives 30\n",
      "F1 : 0.8016877637130801\n",
      "0.812 0.812\n",
      "76\n",
      "(76, 5500)\n",
      "\n",
      "Validated:  0.8157894736842105\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 166 370\n",
      "Overlap (assigned to pos class): 0\n",
      "Traintest pos/neg: 127 127\n",
      "Validation pos/neg: 39 39\n",
      "Traintest mean date pos: 1881.5905511811025 neg: 1881.6692913385828\n",
      "Validation mean date pos 1864.4615384615386 neg: 1862.7692307692307\n",
      "\n",
      "We started with 254 rows in metadata, but\n",
      "lost 1 that were missing in the data folder.\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 126 potential positive instances and\n",
      "127 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1761 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.712\n",
      "words: 1000  reg: 0.001  acc: 0.744\n",
      "words: 1000  reg: 0.003  acc: 0.756\n",
      "words: 2000  reg: 0.001  acc: 0.772\n",
      "words: 3500  reg: 0.1  acc: 0.78\n",
      "words: 5500  reg: 0.1  acc: 0.784\n",
      "words: 5500  reg: 1  acc: 0.788\n",
      "True positives 87\n",
      "True negatives 110\n",
      "False positives 15\n",
      "False negatives 38\n",
      "F1 : 0.7665198237885462\n",
      "0.788 0.788\n",
      "78\n",
      "(78, 5500)\n",
      "\n",
      "Validated:  0.8333333333333334\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 166 370\n",
      "Overlap (assigned to pos class): 0\n",
      "Traintest pos/neg: 127 130\n",
      "Validation pos/neg: 39 39\n",
      "Traintest mean date pos: 1873.7230769230769 neg: 1873.259842519685\n",
      "Validation mean date pos 1890.025641025641 neg: 1890.1538461538462\n",
      "\n",
      "We started with 257 rows in metadata, but\n",
      "lost 1 that were missing in the data folder.\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 126 potential positive instances and\n",
      "130 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1760 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.72\n",
      "words: 1000  reg: 0.001  acc: 0.772\n",
      "words: 1000  reg: 0.003  acc: 0.796\n",
      "words: 1000  reg: 10  acc: 0.8\n",
      "words: 1500  reg: 0.003  acc: 0.804\n",
      "words: 4000  reg: 0.001  acc: 0.808\n",
      "words: 4000  reg: 0.003  acc: 0.812\n",
      "words: 4500  reg: 0.03  acc: 0.816\n",
      "words: 5000  reg: 1  acc: 0.82\n",
      "words: 5000  reg: 10  acc: 0.824\n",
      "True positives 96\n",
      "True negatives 110\n",
      "False positives 15\n",
      "False negatives 29\n",
      "F1 : 0.8135593220338984\n",
      "0.824 0.824\n",
      "78\n",
      "(78, 5000)\n",
      "\n",
      "Validated:  0.8333333333333334\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 166 370\n",
      "Overlap (assigned to pos class): 0\n",
      "Traintest pos/neg: 127 131\n",
      "Validation pos/neg: 39 39\n",
      "Traintest mean date pos: 1875.496183206107 neg: 1873.4803149606298\n",
      "Validation mean date pos 1890.5384615384614 neg: 1889.4358974358975\n",
      "\n",
      "We started with 258 rows in metadata, but\n",
      "lost 1 that were missing in the data folder.\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 126 potential positive instances and\n",
      "131 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1761 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.764\n",
      "words: 1000  reg: 0.001  acc: 0.776\n",
      "words: 1000  reg: 0.003  acc: 0.804\n",
      "words: 1000  reg: 0.01  acc: 0.816\n",
      "True positives 94\n",
      "True negatives 110\n",
      "False positives 15\n",
      "False negatives 31\n",
      "F1 : 0.8034188034188036\n",
      "0.816 0.816\n",
      "78\n",
      "(78, 1000)\n",
      "\n",
      "Validated:  0.7564102564102564\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 166 370\n",
      "Overlap (assigned to pos class): 0\n",
      "Traintest pos/neg: 131 136\n",
      "Validation pos/neg: 35 35\n",
      "Traintest mean date pos: 1872.7205882352941 neg: 1871.6259541984732\n",
      "Validation mean date pos 1896.8 neg: 1898.2\n",
      "\n",
      "We started with 267 rows in metadata, but\n",
      "lost 1 that were missing in the data folder.\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 130 potential positive instances and\n",
      "136 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1764 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[14, 13, 13, 13, 12, 12, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.716\n",
      "words: 1000  reg: 0.001  acc: 0.752\n",
      "words: 1000  reg: 0.003  acc: 0.76\n",
      "words: 1000  reg: 0.01  acc: 0.764\n",
      "words: 1500  reg: 0.001  acc: 0.772\n",
      "words: 1500  reg: 0.003  acc: 0.776\n",
      "words: 1500  reg: 0.1  acc: 0.78\n",
      "words: 1500  reg: 100  acc: 0.784\n",
      "words: 2000  reg: 1  acc: 0.788\n",
      "words: 2500  reg: 0.003  acc: 0.796\n",
      "words: 3500  reg: 0.001  acc: 0.8\n",
      "words: 3500  reg: 0.01  acc: 0.804\n",
      "True positives 91\n",
      "True negatives 110\n",
      "False positives 15\n",
      "False negatives 34\n",
      "F1 : 0.787878787878788\n",
      "0.804 0.804\n",
      "70\n",
      "(70, 3500)\n",
      "\n",
      "Validated:  0.7285714285714285\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 166 370\n",
      "Overlap (assigned to pos class): 0\n",
      "Traintest pos/neg: 127 130\n",
      "Validation pos/neg: 39 39\n",
      "Traintest mean date pos: 1877.0384615384614 neg: 1877.5275590551182\n",
      "Validation mean date pos 1878.6153846153845 neg: 1876.2564102564102\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 127 potential positive instances and\n",
      "130 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1764 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.708\n",
      "words: 1000  reg: 0.001  acc: 0.74\n",
      "words: 1000  reg: 0.003  acc: 0.764\n",
      "words: 1000  reg: 0.01  acc: 0.772\n",
      "words: 1000  reg: 0.03  acc: 0.78\n",
      "True positives 89\n",
      "True negatives 106\n",
      "False positives 19\n",
      "False negatives 36\n",
      "F1 : 0.7639484978540771\n",
      "0.78 0.78\n",
      "../newdata/hvd.hwpn81.fic.tsv\n",
      "77\n",
      "(77, 1000)\n",
      "\n",
      "Validated:  0.8333333333333334\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 166 370\n",
      "Overlap (assigned to pos class): 0\n",
      "Traintest pos/neg: 128 135\n",
      "Validation pos/neg: 38 38\n",
      "Traintest mean date pos: 1880.585185185185 neg: 1878.96875\n",
      "Validation mean date pos 1871.5 neg: 1871.3684210526317\n",
      "\n",
      "We started with 263 rows in metadata, but\n",
      "lost 1 that were missing in the data folder.\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 127 potential positive instances and\n",
      "135 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1764 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 12, 12, 12, 12, 15, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.704\n",
      "words: 1000  reg: 0.001  acc: 0.716\n",
      "words: 1000  reg: 0.003  acc: 0.736\n",
      "words: 1500  reg: 0.001  acc: 0.74\n",
      "words: 2000  reg: 0.003  acc: 0.744\n",
      "words: 2500  reg: 0.001  acc: 0.748\n",
      "words: 3000  reg: 0.001  acc: 0.752\n",
      "words: 3500  reg: 0.001  acc: 0.76\n",
      "words: 4000  reg: 0.001  acc: 0.768\n",
      "words: 5000  reg: 1  acc: 0.772\n",
      "words: 6000  reg: 0.003  acc: 0.776\n",
      "words: 6000  reg: 0.01  acc: 0.784\n",
      "words: 6000  reg: 0.03  acc: 0.792\n",
      "True positives 90\n",
      "True negatives 108\n",
      "False positives 17\n",
      "False negatives 35\n",
      "F1 : 0.7758620689655171\n",
      "0.792 0.792\n",
      "76\n",
      "(76, 6000)\n",
      "\n",
      "Validated:  0.8026315789473685\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 166 370\n",
      "Overlap (assigned to pos class): 0\n",
      "Traintest pos/neg: 127 134\n",
      "Validation pos/neg: 39 39\n",
      "Traintest mean date pos: 1879.313432835821 neg: 1876.5984251968505\n",
      "Validation mean date pos 1879.3333333333333 neg: 1879.2820512820513\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 127 potential positive instances and\n",
      "134 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1761 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 12, 12, 12, 12, 12, 13]\n",
      "[13, 13, 13, 13, 12, 12, 12, 12, 12, 13]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.716\n",
      "words: 1000  reg: 0.001  acc: 0.748\n",
      "words: 1000  reg: 0.003  acc: 0.768\n",
      "words: 1000  reg: 0.01  acc: 0.788\n",
      "words: 1000  reg: 0.03  acc: 0.796\n",
      "words: 3000  reg: 0.01  acc: 0.8\n",
      "words: 3500  reg: 0.003  acc: 0.808\n",
      "words: 3500  reg: 100  acc: 0.812\n",
      "words: 4500  reg: 0.01  acc: 0.816\n",
      "words: 4500  reg: 0.03  acc: 0.828\n",
      "True positives 96\n",
      "True negatives 111\n",
      "False positives 14\n",
      "False negatives 29\n",
      "F1 : 0.8170212765957446\n",
      "0.828 0.828\n",
      "../newdata/hvd.hwpn81.fic.tsv\n",
      "77\n",
      "(77, 4500)\n",
      "\n",
      "Validated:  0.7564102564102564\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 166 370\n",
      "Overlap (assigned to pos class): 0\n",
      "Traintest pos/neg: 126 130\n",
      "Validation pos/neg: 40 40\n",
      "Traintest mean date pos: 1880.8076923076924 neg: 1880.3650793650793\n",
      "Validation mean date pos 1866.3 neg: 1867.35\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 126 potential positive instances and\n",
      "130 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1760 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.74\n",
      "words: 1000  reg: 0.001  acc: 0.752\n",
      "words: 1000  reg: 0.003  acc: 0.76\n",
      "words: 1000  reg: 0.01  acc: 0.772\n",
      "words: 1500  reg: 0.001  acc: 0.776\n",
      "words: 1500  reg: 0.01  acc: 0.792\n",
      "words: 1500  reg: 0.1  acc: 0.796\n",
      "words: 1500  reg: 1  acc: 0.8\n",
      "words: 2000  reg: 0.1  acc: 0.804\n",
      "words: 3000  reg: 1000  acc: 0.808\n",
      "words: 5500  reg: 10  acc: 0.812\n",
      "True positives 95\n",
      "True negatives 108\n",
      "False positives 17\n",
      "False negatives 30\n",
      "F1 : 0.8016877637130801\n",
      "0.812 0.812\n",
      "../newdata/hvd.hwpn81.fic.tsv\n",
      "79\n",
      "(79, 5500)\n",
      "\n",
      "Validated:  0.7125\n"
     ]
    }
   ],
   "source": [
    "train_and_validate('BoWGothic', {'lochorror', 'pbgothic', 'locghost', 'stangothic', 'chihorror'},\n",
    "        {'random', 'chirandom'}, 125, '../newdata/', '../meta/finalmeta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 175 347\n",
      "Overlap (assigned to pos class): 23\n",
      "Traintest pos/neg: 126 131\n",
      "Validation pos/neg: 72 72\n",
      "Traintest mean date pos: 1942.3358778625955 neg: 1943.388888888889\n",
      "Validation mean date pos 1929.1666666666667 neg: 1929.4305555555557\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 126 potential positive instances and\n",
      "131 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1771 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 12, 12, 12, 13, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.8\n",
      "words: 1000  reg: 0.001  acc: 0.832\n",
      "words: 1000  reg: 0.003  acc: 0.836\n",
      "words: 1000  reg: 0.01  acc: 0.852\n",
      "words: 1000  reg: 0.03  acc: 0.856\n",
      "words: 1000  reg: 0.1  acc: 0.86\n",
      "words: 1000  reg: 1  acc: 0.864\n",
      "words: 1500  reg: 0.003  acc: 0.88\n",
      "words: 1500  reg: 0.01  acc: 0.888\n",
      "words: 1500  reg: 0.03  acc: 0.892\n",
      "words: 2500  reg: 0.03  acc: 0.904\n",
      "words: 3000  reg: 1  acc: 0.912\n",
      "words: 3500  reg: 1  acc: 0.916\n",
      "words: 3500  reg: 10  acc: 0.92\n",
      "True positives 113\n",
      "True negatives 117\n",
      "False positives 8\n",
      "False negatives 12\n",
      "F1 : 0.9186991869918698\n",
      "0.92 0.92\n",
      "../newdata/inu.30000042750632.fic.tsv\n",
      "../newdata/dul1.ark+=13960=t95723c7j.fic.tsv\n",
      "142\n",
      "(142, 3500)\n",
      "\n",
      "Validated:  0.8819444444444444\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 175 347\n",
      "Overlap (assigned to pos class): 23\n",
      "Traintest pos/neg: 126 127\n",
      "Validation pos/neg: 72 72\n",
      "Traintest mean date pos: 1945.1338582677165 neg: 1945.611111111111\n",
      "Validation mean date pos 1924.986111111111 neg: 1925.5416666666667\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 126 potential positive instances and\n",
      "127 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1771 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.804\n",
      "words: 1000  reg: 0.001  acc: 0.816\n",
      "words: 1000  reg: 0.003  acc: 0.836\n",
      "words: 1000  reg: 0.01  acc: 0.852\n",
      "words: 1000  reg: 0.03  acc: 0.864\n",
      "words: 1500  reg: 0.003  acc: 0.876\n",
      "words: 1500  reg: 0.01  acc: 0.88\n",
      "words: 1500  reg: 0.1  acc: 0.884\n",
      "words: 2000  reg: 300  acc: 0.888\n",
      "words: 2500  reg: 10  acc: 0.892\n",
      "words: 2500  reg: 300  acc: 0.896\n",
      "words: 4000  reg: 100  acc: 0.9\n",
      "words: 5500  reg: 100  acc: 0.904\n",
      "True positives 110\n",
      "True negatives 116\n",
      "False positives 9\n",
      "False negatives 15\n",
      "F1 : 0.901639344262295\n",
      "0.904 0.904\n",
      "../newdata/inu.30000042750632.fic.tsv\n",
      "../newdata/dul1.ark+=13960=t95723c7j.fic.tsv\n",
      "142\n",
      "(142, 5500)\n",
      "\n",
      "Validated:  0.8888888888888888\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 175 347\n",
      "Overlap (assigned to pos class): 23\n",
      "Traintest pos/neg: 127 133\n",
      "Validation pos/neg: 71 71\n",
      "Traintest mean date pos: 1939.9398496240601 neg: 1942.2992125984251\n",
      "Validation mean date pos 1930.9154929577464 neg: 1931.1830985915492\n",
      "\n",
      "We started with 260 rows in metadata, but\n",
      "lost 1 that were missing in the data folder.\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 126 potential positive instances and\n",
      "133 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1818 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 12, 12, 13, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.776\n",
      "words: 1000  reg: 0.001  acc: 0.828\n",
      "words: 1000  reg: 0.003  acc: 0.856\n",
      "words: 1500  reg: 0.03  acc: 0.86\n",
      "words: 2000  reg: 0.003  acc: 0.864\n",
      "words: 3000  reg: 300  acc: 0.868\n",
      "words: 4000  reg: 10  acc: 0.88\n",
      "words: 4500  reg: 100  acc: 0.888\n",
      "words: 4500  reg: 300  acc: 0.892\n",
      "words: 6000  reg: 100  acc: 0.896\n",
      "True positives 112\n",
      "True negatives 112\n",
      "False positives 13\n",
      "False negatives 13\n",
      "F1 : 0.8960000000000001\n",
      "0.896 0.896\n",
      "../newdata/dul1.ark+=13960=t95723c7j.fic.tsv\n",
      "141\n",
      "(141, 6000)\n",
      "\n",
      "Validated:  0.9366197183098591\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 175 347\n",
      "Overlap (assigned to pos class): 23\n",
      "Traintest pos/neg: 129 137\n",
      "Validation pos/neg: 69 69\n",
      "Traintest mean date pos: 1937.021897810219 neg: 1939.3100775193798\n",
      "Validation mean date pos 1935.9710144927535 neg: 1936.4492753623188\n",
      "\n",
      "We started with 266 rows in metadata, but\n",
      "lost 1 that were missing in the data folder.\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 128 potential positive instances and\n",
      "137 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1771 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.792\n",
      "words: 1000  reg: 0.001  acc: 0.832\n",
      "words: 1000  reg: 0.003  acc: 0.844\n",
      "words: 1000  reg: 0.01  acc: 0.856\n",
      "words: 1000  reg: 0.1  acc: 0.86\n",
      "words: 1500  reg: 1  acc: 0.864\n",
      "words: 2000  reg: 10  acc: 0.868\n",
      "words: 2500  reg: 0.003  acc: 0.872\n",
      "words: 3000  reg: 0.01  acc: 0.88\n",
      "words: 3000  reg: 0.03  acc: 0.884\n",
      "words: 3000  reg: 100  acc: 0.888\n",
      "words: 3500  reg: 0.01  acc: 0.892\n",
      "words: 3500  reg: 0.03  acc: 0.896\n",
      "True positives 111\n",
      "True negatives 113\n",
      "False positives 12\n",
      "False negatives 14\n",
      "F1 : 0.8951612903225806\n",
      "0.896 0.896\n",
      "../newdata/inu.30000042750632.fic.tsv\n",
      "137\n",
      "(137, 3500)\n",
      "\n",
      "Validated:  0.8768115942028986\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 175 347\n",
      "Overlap (assigned to pos class): 23\n",
      "Traintest pos/neg: 126 134\n",
      "Validation pos/neg: 72 72\n",
      "Traintest mean date pos: 1937.44776119403 neg: 1940.2301587301588\n",
      "Validation mean date pos 1934.0277777777778 neg: 1934.9583333333333\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 126 potential positive instances and\n",
      "134 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1818 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.808\n",
      "words: 1000  reg: 0.001  acc: 0.852\n",
      "words: 1000  reg: 0.01  acc: 0.856\n",
      "words: 1000  reg: 0.1  acc: 0.86\n",
      "words: 2000  reg: 0.001  acc: 0.864\n",
      "words: 2000  reg: 0.003  acc: 0.868\n",
      "words: 2500  reg: 0.001  acc: 0.872\n",
      "words: 2500  reg: 100  acc: 0.876\n",
      "words: 2500  reg: 300  acc: 0.884\n",
      "words: 3000  reg: 100  acc: 0.892\n",
      "words: 4000  reg: 0.003  acc: 0.9\n",
      "words: 5500  reg: 0.001  acc: 0.904\n",
      "words: 5500  reg: 0.003  acc: 0.908\n",
      "True positives 108\n",
      "True negatives 119\n",
      "False positives 6\n",
      "False negatives 17\n",
      "F1 : 0.9037656903765691\n",
      "0.908 0.908\n",
      "../newdata/inu.30000042750632.fic.tsv\n",
      "../newdata/dul1.ark+=13960=t95723c7j.fic.tsv\n",
      "142\n",
      "(142, 5500)\n",
      "\n",
      "Validated:  0.8680555555555556\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 175 347\n",
      "Overlap (assigned to pos class): 23\n",
      "Traintest pos/neg: 126 133\n",
      "Validation pos/neg: 72 72\n",
      "Traintest mean date pos: 1934.3533834586467 neg: 1936.7063492063492\n",
      "Validation mean date pos 1940.986111111111 neg: 1941.125\n",
      "\n",
      "We started with 259 rows in metadata, but\n",
      "lost 2 that were missing in the data folder.\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 124 potential positive instances and\n",
      "133 potential negative instances. Choosing only\n",
      "124 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "248 volumes range in date from 1818 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 12, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 12, 12, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.8064516129032258\n",
      "words: 1000  reg: 0.001  acc: 0.8306451612903226\n",
      "words: 1000  reg: 0.003  acc: 0.8467741935483871\n",
      "words: 1000  reg: 0.01  acc: 0.8629032258064516\n",
      "words: 1500  reg: 0.01  acc: 0.8669354838709677\n",
      "words: 2000  reg: 0.003  acc: 0.875\n",
      "words: 2000  reg: 0.01  acc: 0.8790322580645161\n",
      "words: 2000  reg: 0.03  acc: 0.8830645161290323\n",
      "words: 2000  reg: 1  acc: 0.8870967741935484\n",
      "words: 2500  reg: 0.003  acc: 0.8951612903225806\n",
      "words: 2500  reg: 0.01  acc: 0.8991935483870968\n",
      "words: 3000  reg: 1  acc: 0.9032258064516129\n",
      "words: 3000  reg: 300  acc: 0.907258064516129\n",
      "words: 4000  reg: 0.01  acc: 0.9153225806451613\n",
      "True positives 110\n",
      "True negatives 117\n",
      "False positives 7\n",
      "False negatives 14\n",
      "F1 : 0.9128630705394192\n",
      "0.9153225806451613 0.915322580645\n",
      "144\n",
      "(144, 4000)\n",
      "\n",
      "Validated:  0.8472222222222222\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 175 347\n",
      "Overlap (assigned to pos class): 23\n",
      "Traintest pos/neg: 129 132\n",
      "Validation pos/neg: 69 69\n",
      "Traintest mean date pos: 1935.0227272727273 neg: 1936.968992248062\n",
      "Validation mean date pos 1940.4782608695652 neg: 1940.8260869565217\n",
      "\n",
      "We started with 261 rows in metadata, but\n",
      "lost 2 that were missing in the data folder.\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 127 potential positive instances and\n",
      "132 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1771 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 14, 13, 13, 12, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.816\n",
      "words: 1000  reg: 0.001  acc: 0.852\n",
      "words: 1000  reg: 0.003  acc: 0.856\n",
      "words: 1000  reg: 0.01  acc: 0.872\n",
      "words: 2000  reg: 0.01  acc: 0.884\n",
      "words: 2500  reg: 0.003  acc: 0.888\n",
      "words: 2500  reg: 0.01  acc: 0.892\n",
      "words: 3500  reg: 100  acc: 0.896\n",
      "words: 4500  reg: 1  acc: 0.9\n",
      "words: 4500  reg: 100  acc: 0.904\n",
      "words: 4500  reg: 300  acc: 0.908\n",
      "words: 6000  reg: 10  acc: 0.912\n",
      "True positives 109\n",
      "True negatives 119\n",
      "False positives 6\n",
      "False negatives 16\n",
      "F1 : 0.9083333333333333\n",
      "0.912 0.912\n",
      "138\n",
      "(138, 6000)\n",
      "\n",
      "Validated:  0.9057971014492754\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 175 347\n",
      "Overlap (assigned to pos class): 23\n",
      "Traintest pos/neg: 126 132\n",
      "Validation pos/neg: 72 72\n",
      "Traintest mean date pos: 1936.2121212121212 neg: 1936.9126984126983\n",
      "Validation mean date pos 1940.0555555555557 neg: 1940.763888888889\n",
      "\n",
      "We started with 258 rows in metadata, but\n",
      "lost 2 that were missing in the data folder.\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 124 potential positive instances and\n",
      "132 potential negative instances. Choosing only\n",
      "124 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "248 volumes range in date from 1771 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 12, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 12, 12, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.8266129032258065\n",
      "words: 1000  reg: 0.001  acc: 0.8629032258064516\n",
      "words: 1000  reg: 0.003  acc: 0.8830645161290323\n",
      "words: 1000  reg: 0.1  acc: 0.8870967741935484\n",
      "words: 1000  reg: 1  acc: 0.8911290322580645\n",
      "words: 1500  reg: 100  acc: 0.8951612903225806\n",
      "words: 2500  reg: 1  acc: 0.8991935483870968\n",
      "words: 4500  reg: 100  acc: 0.9032258064516129\n",
      "words: 5000  reg: 0.01  acc: 0.9112903225806451\n",
      "words: 5000  reg: 10  acc: 0.9193548387096774\n",
      "words: 6000  reg: 1000  acc: 0.9233870967741935\n",
      "True positives 114\n",
      "True negatives 115\n",
      "False positives 9\n",
      "False negatives 10\n",
      "F1 : 0.923076923076923\n",
      "0.9233870967741935 0.923387096774\n",
      "144\n",
      "(144, 6000)\n",
      "\n",
      "Validated:  0.8888888888888888\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 175 347\n",
      "Overlap (assigned to pos class): 23\n",
      "Traintest pos/neg: 126 128\n",
      "Validation pos/neg: 72 72\n",
      "Traintest mean date pos: 1943.0546875 neg: 1943.8412698412699\n",
      "Validation mean date pos 1928.361111111111 neg: 1928.638888888889\n",
      "\n",
      "We started with 254 rows in metadata, but\n",
      "lost 1 that were missing in the data folder.\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 125 potential positive instances and\n",
      "128 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1827 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.828\n",
      "words: 1000  reg: 0.001  acc: 0.876\n",
      "words: 1000  reg: 0.01  acc: 0.888\n",
      "words: 1000  reg: 0.03  acc: 0.892\n",
      "words: 1000  reg: 0.1  acc: 0.9\n",
      "words: 1000  reg: 100  acc: 0.904\n",
      "words: 4500  reg: 0.03  acc: 0.908\n",
      "words: 4500  reg: 0.1  acc: 0.912\n",
      "words: 5000  reg: 0.1  acc: 0.916\n",
      "True positives 113\n",
      "True negatives 116\n",
      "False positives 9\n",
      "False negatives 12\n",
      "F1 : 0.9149797570850203\n",
      "0.916 0.916\n",
      "../newdata/dul1.ark+=13960=t95723c7j.fic.tsv\n",
      "143\n",
      "(143, 5000)\n",
      "\n",
      "Validated:  0.8541666666666666\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 175 347\n",
      "Overlap (assigned to pos class): 23\n",
      "Traintest pos/neg: 126 131\n",
      "Validation pos/neg: 72 72\n",
      "Traintest mean date pos: 1937.3664122137404 neg: 1939.952380952381\n",
      "Validation mean date pos 1935.3333333333333 neg: 1935.4444444444443\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 126 potential positive instances and\n",
      "131 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1771 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.78\n",
      "words: 1000  reg: 0.001  acc: 0.824\n",
      "words: 1000  reg: 0.003  acc: 0.836\n",
      "words: 1000  reg: 0.01  acc: 0.844\n",
      "words: 1000  reg: 0.03  acc: 0.848\n",
      "words: 1000  reg: 100  acc: 0.856\n",
      "words: 2000  reg: 0.003  acc: 0.86\n",
      "words: 3500  reg: 0.01  acc: 0.868\n",
      "words: 3500  reg: 0.1  acc: 0.872\n",
      "words: 6000  reg: 0.01  acc: 0.876\n",
      "True positives 103\n",
      "True negatives 116\n",
      "False positives 9\n",
      "False negatives 22\n",
      "F1 : 0.8691983122362869\n",
      "0.876 0.876\n",
      "../newdata/inu.30000042750632.fic.tsv\n",
      "../newdata/dul1.ark+=13960=t95723c7j.fic.tsv\n",
      "142\n",
      "(142, 6000)\n",
      "\n",
      "Validated:  0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "train_and_validate('BoWSF', {'anatscifi', 'locscifi', 'chiscifi', 'femscifi'},\n",
    "        {'random', 'chirandom'}, 125, '../newdata/', '../meta/finalmeta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 206 327\n",
      "Overlap (assigned to pos class): 43\n",
      "Traintest pos/neg: 126 128\n",
      "Validation pos/neg: 123 123\n",
      "Traintest mean date pos: 1928.3046875 neg: 1929.468253968254\n",
      "Validation mean date pos 1932.5934959349593 neg: 1932.4552845528456\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 126 potential positive instances and\n",
      "128 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1832 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 12, 12, 12, 12, 13, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.844\n",
      "words: 1000  reg: 0.001  acc: 0.88\n",
      "words: 1000  reg: 0.003  acc: 0.896\n",
      "words: 1000  reg: 0.03  acc: 0.9\n",
      "words: 2000  reg: 0.1  acc: 0.904\n",
      "words: 3000  reg: 0.001  acc: 0.908\n",
      "words: 3000  reg: 0.003  acc: 0.916\n",
      "words: 3500  reg: 0.01  acc: 0.92\n",
      "True positives 118\n",
      "True negatives 112\n",
      "False positives 13\n",
      "False negatives 7\n",
      "F1 : 0.921875\n",
      "0.92 0.92\n",
      "246\n",
      "(246, 3500)\n",
      "\n",
      "Validated:  0.959349593495935\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 206 327\n",
      "Overlap (assigned to pos class): 43\n",
      "Traintest pos/neg: 126 126\n",
      "Validation pos/neg: 123 123\n",
      "Traintest mean date pos: 1930.7777777777778 neg: 1930.2698412698412\n",
      "Validation mean date pos 1931.3577235772357 neg: 1931.6341463414635\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 126 potential positive instances and\n",
      "126 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1841 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.848\n",
      "words: 1000  reg: 0.001  acc: 0.904\n",
      "words: 1000  reg: 0.003  acc: 0.92\n",
      "words: 1000  reg: 0.01  acc: 0.928\n",
      "words: 1500  reg: 0.003  acc: 0.932\n",
      "words: 1500  reg: 0.01  acc: 0.936\n",
      "words: 2500  reg: 0.1  acc: 0.94\n",
      "words: 4500  reg: 100  acc: 0.944\n",
      "True positives 116\n",
      "True negatives 120\n",
      "False positives 5\n",
      "False negatives 9\n",
      "F1 : 0.943089430894309\n",
      "0.944 0.944\n",
      "246\n",
      "(246, 4500)\n",
      "\n",
      "Validated:  0.9146341463414634\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 206 327\n",
      "Overlap (assigned to pos class): 43\n",
      "Traintest pos/neg: 127 131\n",
      "Validation pos/neg: 122 122\n",
      "Traintest mean date pos: 1929.1145038167938 neg: 1932.5984251968505\n",
      "Validation mean date pos 1928.5573770491803 neg: 1929.22131147541\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 127 potential positive instances and\n",
      "131 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1838 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 12, 12, 12, 13, 12, 12]\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.836\n",
      "words: 1000  reg: 0.001  acc: 0.86\n",
      "words: 1000  reg: 0.003  acc: 0.876\n",
      "words: 1000  reg: 0.03  acc: 0.884\n",
      "words: 1500  reg: 0.001  acc: 0.888\n",
      "words: 2000  reg: 0.003  acc: 0.896\n",
      "words: 2000  reg: 0.01  acc: 0.9\n",
      "words: 4500  reg: 0.003  acc: 0.904\n",
      "words: 5500  reg: 0.003  acc: 0.912\n",
      "True positives 112\n",
      "True negatives 116\n",
      "False positives 9\n",
      "False negatives 13\n",
      "F1 : 0.9105691056910571\n",
      "0.912 0.912\n",
      "244\n",
      "(244, 5500)\n",
      "\n",
      "Validated:  0.9426229508196722\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 206 327\n",
      "Overlap (assigned to pos class): 43\n",
      "Traintest pos/neg: 126 130\n",
      "Validation pos/neg: 123 123\n",
      "Traintest mean date pos: 1932.1846153846154 neg: 1933.8492063492063\n",
      "Validation mean date pos 1926.6910569105692 neg: 1927.9674796747968\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 126 potential positive instances and\n",
      "130 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1832 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 12, 12, 14, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.844\n",
      "words: 1000  reg: 0.001  acc: 0.884\n",
      "words: 1000  reg: 0.003  acc: 0.892\n",
      "words: 1000  reg: 0.03  acc: 0.896\n",
      "words: 1000  reg: 0.1  acc: 0.904\n",
      "True positives 111\n",
      "True negatives 115\n",
      "False positives 10\n",
      "False negatives 14\n",
      "F1 : 0.9024390243902439\n",
      "0.904 0.904\n",
      "246\n",
      "(246, 1000)\n",
      "\n",
      "Validated:  0.9105691056910569\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 206 327\n",
      "Overlap (assigned to pos class): 43\n",
      "Traintest pos/neg: 126 129\n",
      "Validation pos/neg: 123 123\n",
      "Traintest mean date pos: 1921.891472868217 neg: 1924.0238095238096\n",
      "Validation mean date pos 1937.6341463414635 neg: 1938.0325203252032\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 126 potential positive instances and\n",
      "129 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1829 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.824\n",
      "words: 1000  reg: 0.001  acc: 0.868\n",
      "words: 1000  reg: 0.003  acc: 0.876\n",
      "words: 1000  reg: 0.01  acc: 0.888\n",
      "words: 1000  reg: 0.03  acc: 0.892\n",
      "words: 1500  reg: 0.001  acc: 0.9\n",
      "words: 2000  reg: 0.001  acc: 0.908\n",
      "words: 2000  reg: 0.003  acc: 0.916\n",
      "words: 3000  reg: 0.001  acc: 0.92\n",
      "words: 3500  reg: 0.001  acc: 0.924\n",
      "words: 4500  reg: 100  acc: 0.928\n",
      "True positives 116\n",
      "True negatives 116\n",
      "False positives 9\n",
      "False negatives 9\n",
      "F1 : 0.928\n",
      "0.928 0.928\n",
      "246\n",
      "(246, 4500)\n",
      "\n",
      "Validated:  0.9186991869918699\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 206 327\n",
      "Overlap (assigned to pos class): 43\n",
      "Traintest pos/neg: 126 130\n",
      "Validation pos/neg: 123 123\n",
      "Traintest mean date pos: 1924.176923076923 neg: 1925.595238095238\n",
      "Validation mean date pos 1936.4878048780488 neg: 1936.4227642276423\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 126 potential positive instances and\n",
      "130 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1832 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 12, 13, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.868\n",
      "words: 1000  reg: 0.001  acc: 0.888\n",
      "words: 1000  reg: 0.003  acc: 0.892\n",
      "words: 1500  reg: 0.003  acc: 0.9\n",
      "words: 2000  reg: 0.01  acc: 0.912\n",
      "words: 2000  reg: 0.03  acc: 0.916\n",
      "words: 4000  reg: 0.003  acc: 0.92\n",
      "True positives 114\n",
      "True negatives 116\n",
      "False positives 9\n",
      "False negatives 11\n",
      "F1 : 0.9193548387096775\n",
      "0.92 0.92\n",
      "246\n",
      "(246, 4000)\n",
      "\n",
      "Validated:  0.926829268292683\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 206 327\n",
      "Overlap (assigned to pos class): 43\n",
      "Traintest pos/neg: 127 128\n",
      "Validation pos/neg: 122 122\n",
      "Traintest mean date pos: 1931.1953125 neg: 1931.1732283464567\n",
      "Validation mean date pos 1931.2459016393443 neg: 1930.704918032787\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 127 potential positive instances and\n",
      "128 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1829 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 12, 12, 13, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.84\n",
      "words: 1000  reg: 0.001  acc: 0.872\n",
      "words: 1000  reg: 0.003  acc: 0.892\n",
      "words: 1000  reg: 0.01  acc: 0.896\n",
      "words: 1000  reg: 0.03  acc: 0.904\n",
      "words: 2000  reg: 0.01  acc: 0.908\n",
      "words: 4500  reg: 0.003  acc: 0.912\n",
      "words: 5000  reg: 1  acc: 0.916\n",
      "words: 5000  reg: 10  acc: 0.92\n",
      "words: 5500  reg: 10  acc: 0.924\n",
      "True positives 117\n",
      "True negatives 114\n",
      "False positives 11\n",
      "False negatives 8\n",
      "F1 : 0.924901185770751\n",
      "0.924 0.924\n",
      "244\n",
      "(244, 5500)\n",
      "\n",
      "Validated:  0.9139344262295082\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 206 327\n",
      "Overlap (assigned to pos class): 43\n",
      "Traintest pos/neg: 126 131\n",
      "Validation pos/neg: 123 123\n",
      "Traintest mean date pos: 1927.6259541984732 neg: 1930.3095238095239\n",
      "Validation mean date pos 1931.3089430894308 neg: 1931.5934959349593\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 126 potential positive instances and\n",
      "131 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1829 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 12, 12, 12, 12, 12, 13]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.828\n",
      "words: 1000  reg: 0.001  acc: 0.884\n",
      "words: 1000  reg: 0.003  acc: 0.892\n",
      "words: 1000  reg: 0.01  acc: 0.896\n",
      "words: 1000  reg: 0.03  acc: 0.904\n",
      "words: 1000  reg: 0.1  acc: 0.908\n",
      "words: 1000  reg: 1  acc: 0.916\n",
      "True positives 117\n",
      "True negatives 112\n",
      "False positives 13\n",
      "False negatives 8\n",
      "F1 : 0.9176470588235294\n",
      "0.916 0.916\n",
      "246\n",
      "(246, 1000)\n",
      "\n",
      "Validated:  0.926829268292683\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 206 327\n",
      "Overlap (assigned to pos class): 43\n",
      "Traintest pos/neg: 126 127\n",
      "Validation pos/neg: 123 123\n",
      "Traintest mean date pos: 1934.4881889763778 neg: 1935.111111111111\n",
      "Validation mean date pos 1926.4552845528456 neg: 1926.6747967479675\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 126 potential positive instances and\n",
      "127 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1829 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.848\n",
      "words: 1000  reg: 0.001  acc: 0.888\n",
      "words: 1500  reg: 0.003  acc: 0.896\n",
      "words: 1500  reg: 0.01  acc: 0.912\n",
      "True positives 113\n",
      "True negatives 115\n",
      "False positives 10\n",
      "False negatives 12\n",
      "F1 : 0.9112903225806451\n",
      "0.912 0.912\n",
      "246\n",
      "(246, 1500)\n",
      "\n",
      "Validated:  0.926829268292683\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 206 327\n",
      "Overlap (assigned to pos class): 43\n",
      "Traintest pos/neg: 127 127\n",
      "Validation pos/neg: 122 122\n",
      "Traintest mean date pos: 1931.5905511811025 neg: 1931.9133858267717\n",
      "Validation mean date pos 1930.0737704918033 neg: 1929.9344262295083\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 127 potential positive instances and\n",
      "127 potential negative instances. Choosing only\n",
      "125 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "250 volumes range in date from 1832 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "[13, 13, 13, 13, 13, 12, 12, 12, 12, 12]\n",
      "\n",
      "words: 1000  reg: 0.0001  acc: 0.868\n",
      "words: 1000  reg: 0.001  acc: 0.896\n",
      "words: 1000  reg: 0.003  acc: 0.904\n",
      "words: 1000  reg: 0.03  acc: 0.912\n",
      "words: 1500  reg: 0.003  acc: 0.916\n",
      "words: 3000  reg: 0.001  acc: 0.92\n",
      "words: 5000  reg: 0.01  acc: 0.928\n",
      "words: 5000  reg: 10  acc: 0.932\n",
      "True positives 114\n",
      "True negatives 119\n",
      "False positives 6\n",
      "False negatives 11\n",
      "F1 : 0.9306122448979591\n",
      "0.932 0.932\n",
      "244\n",
      "(244, 5000)\n",
      "\n",
      "Validated:  0.930327868852459\n"
     ]
    }
   ],
   "source": [
    "train_and_validate('BoWMystery', {'locdetective', 'locdetmyst', 'chimyst', 'det100'},\n",
    "        {'random', 'chirandom'}, 125, '../newdata/', '../meta/finalmeta.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trials on reduced data\n",
    "\n",
    "The same models run on a corpus down-sampled to 5% of the data (each word instance had a 5% chance of being recorded) and 80 instead of 125 volumes.\n",
    "\n",
    "We used this alternate version of **tune_a_model():**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tune_a_model(name, tags4positive, tags4negative, sizecap, sourcefolder, metadatapath):\n",
    "    '''\n",
    "    This tunes a model through gridsearch, and puts the resulting model in a ../temp\n",
    "    folder, where it can be retrieved\n",
    "    '''\n",
    "\n",
    "    vocabpath = '../lexica/' + name + '.txt'\n",
    "    modeloutpath = '../temp/' + name + '.csv'\n",
    "\n",
    "    c_range = [.00001, .0001, .001, .003, .01, .03, 0.1, 1, 10, 100, 300, 1000]\n",
    "    featurestart = 10\n",
    "    featureend = 1500\n",
    "    featurestep = 100\n",
    "    modelparams = 'logistic', 10, featurestart, featureend, featurestep, c_range\n",
    "    forbiddenwords = {}\n",
    "    floor = 1700\n",
    "    ceiling = 2020\n",
    "\n",
    "    metadata, masterdata, classvector, classdictionary, orderedIDs, authormatches, vocablist = versatiletrainer2.get_simple_data(sourcefolder, metadatapath, vocabpath, tags4positive, tags4negative, sizecap, extension = '.fic.tsv', excludebelow = floor, excludeabove = ceiling,\n",
    "        forbid4positive = {'drop'}, forbid4negative = {'drop'}, force_even_distribution = False, forbiddenwords = forbiddenwords)\n",
    "\n",
    "    matrix, maxaccuracy, metadata, coefficientuples, features4max, best_regularization_coef = versatiletrainer2.tune_a_model(metadata, masterdata, classvector, classdictionary, orderedIDs, authormatches,\n",
    "        vocablist, tags4positive, tags4negative, modelparams, name, modeloutpath)\n",
    "\n",
    "    meandate = int(round(np.sum(metadata.firstpub) / len(metadata.firstpub)))\n",
    "    floor = np.min(metadata.firstpub)\n",
    "    ceiling = np.max(metadata.firstpub)\n",
    "\n",
    "    os.remove(vocabpath)\n",
    "    \n",
    "    return floor, ceiling, meandate, maxaccuracy, features4max, best_regularization_coef, modeloutpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 166 370\n",
      "Overlap (assigned to pos class): 0\n",
      "Traintest pos/neg: 43 51\n",
      "Validation to match: 121 neg avail: 319\n",
      "Traintest mean date pos: 1881.5882352941176 neg: 1884.4418604651162\n",
      "Validation mean date pos 1877.3543307086613 neg: 1874.4876033057851\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 43 potential positive instances and\n",
      "51 potential negative instances. Choosing only\n",
      "40 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "80 volumes range in date from 1764 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[5, 4, 4, 4, 4, 4, 4, 4, 4, 3]\n",
      "[5, 4, 4, 4, 4, 4, 4, 4, 4, 3]\n",
      "\n",
      "words: 10  reg: 1e-05  acc: 0.5875\n",
      "words: 110  reg: 1e-05  acc: 0.6\n",
      "words: 110  reg: 0.01  acc: 0.6375\n",
      "words: 110  reg: 0.03  acc: 0.65\n",
      "words: 110  reg: 0.1  acc: 0.6875\n",
      "True positives 29\n",
      "True negatives 26\n",
      "False positives 14\n",
      "False negatives 11\n",
      "F1 : 0.6987951807228916\n",
      "0.6875 0.6875\n",
      "../reduced_data/hvd.hwpn81.fic.tsv\n",
      "247\n",
      "(247, 110)\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 166 370\n",
      "Overlap (assigned to pos class): 0\n",
      "Traintest pos/neg: 44 52\n",
      "Validation to match: 120 neg avail: 314\n",
      "Traintest mean date pos: 1887.1538461538462 neg: 1889.7272727272727\n",
      "Validation mean date pos 1874.792 neg: 1872.4666666666667\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 44 potential positive instances and\n",
      "52 potential negative instances. Choosing only\n",
      "40 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "80 volumes range in date from 1788 to 1988.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[5, 5, 4, 4, 4, 4, 4, 4, 3, 3]\n",
      "[5, 4, 4, 5, 4, 4, 4, 4, 3, 3]\n",
      "\n",
      "words: 10  reg: 1e-05  acc: 0.625\n",
      "words: 10  reg: 0.03  acc: 0.6625\n",
      "words: 10  reg: 1  acc: 0.7375\n",
      "True positives 28\n",
      "True negatives 31\n",
      "False positives 9\n",
      "False negatives 12\n",
      "F1 : 0.7272727272727273\n",
      "0.7375 0.7375\n",
      "../reduced_data/hvd.hwpn81.fic.tsv\n",
      "244\n",
      "(244, 10)\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 166 370\n",
      "Overlap (assigned to pos class): 0\n",
      "Traintest pos/neg: 41 47\n",
      "Validation to match: 119 neg avail: 322\n",
      "Traintest mean date pos: 1874.212765957447 neg: 1878.5121951219512\n",
      "Validation mean date pos 1879.25 neg: 1877.109243697479\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 41 potential positive instances and\n",
      "47 potential negative instances. Choosing only\n",
      "40 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "80 volumes range in date from 1785 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[5, 4, 4, 4, 4, 4, 4, 4, 4, 3]\n",
      "[4, 6, 4, 6, 3, 5, 3, 3, 3, 3]\n",
      "\n",
      "words: 10  reg: 1e-05  acc: 0.625\n",
      "words: 10  reg: 0.0001  acc: 0.6375\n",
      "words: 110  reg: 0.1  acc: 0.6625\n",
      "words: 710  reg: 1  acc: 0.675\n",
      "True positives 24\n",
      "True negatives 30\n",
      "False positives 10\n",
      "False negatives 16\n",
      "F1 : 0.6486486486486486\n",
      "0.675 0.675\n",
      "../reduced_data/hvd.hwpn81.fic.tsv\n",
      "242\n",
      "(242, 710)\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 166 370\n",
      "Overlap (assigned to pos class): 0\n",
      "Traintest pos/neg: 41 47\n",
      "Validation to match: 120 neg avail: 318\n",
      "Traintest mean date pos: 1890.2978723404256 neg: 1887.9268292682927\n",
      "Validation mean date pos 1874.4634146341464 neg: 1873.025\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 41 potential positive instances and\n",
      "47 potential negative instances. Choosing only\n",
      "40 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "80 volumes range in date from 1764 to 1987.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4, 4, 5, 4, 4, 4, 4, 4, 4, 3]\n",
      "\n",
      "words: 10  reg: 1e-05  acc: 0.5375\n",
      "words: 10  reg: 0.01  acc: 0.5625\n",
      "words: 10  reg: 0.03  acc: 0.575\n",
      "words: 110  reg: 0.03  acc: 0.6125\n",
      "words: 110  reg: 0.1  acc: 0.675\n",
      "words: 110  reg: 10  acc: 0.6875\n",
      "True positives 27\n",
      "True negatives 28\n",
      "False positives 12\n",
      "False negatives 13\n",
      "F1 : 0.6835443037974683\n",
      "0.6875 0.6875\n",
      "../reduced_data/hvd.hwpn81.fic.tsv\n",
      "242\n",
      "(242, 110)\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 166 370\n",
      "Overlap (assigned to pos class): 0\n",
      "Traintest pos/neg: 41 44\n",
      "Validation to match: 125 neg avail: 325\n",
      "Traintest mean date pos: 1861.1363636363637 neg: 1863.9756097560976\n",
      "Validation mean date pos 1885.0597014925372 neg: 1881.576\n",
      "\n",
      "We started with 85 rows in metadata, but\n",
      "lost 1 that were missing in the data folder.\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 40 potential positive instances and\n",
      "44 potential negative instances. Choosing only\n",
      "40 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "80 volumes range in date from 1785 to 1981.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4, 6, 5, 4, 3, 4, 3, 3, 5, 3]\n",
      "\n",
      "words: 10  reg: 1e-05  acc: 0.4\n",
      "words: 10  reg: 0.001  acc: 0.4125\n",
      "words: 10  reg: 0.03  acc: 0.4625\n",
      "words: 10  reg: 0.1  acc: 0.4875\n",
      "words: 10  reg: 1  acc: 0.525\n",
      "words: 210  reg: 0.1  acc: 0.5375\n",
      "words: 310  reg: 0.03  acc: 0.5875\n",
      "words: 310  reg: 0.1  acc: 0.6125\n",
      "words: 310  reg: 100  acc: 0.625\n",
      "True positives 25\n",
      "True negatives 25\n",
      "False positives 15\n",
      "False negatives 15\n",
      "F1 : 0.625\n",
      "0.625 0.625\n",
      "259\n",
      "(259, 310)\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 166 370\n",
      "Overlap (assigned to pos class): 0\n",
      "Traintest pos/neg: 44 50\n",
      "Validation to match: 122 neg avail: 319\n",
      "Traintest mean date pos: 1872.14 neg: 1877.159090909091\n",
      "Validation mean date pos 1878.2983870967741 neg: 1877.2540983606557\n",
      "\n",
      "We started with 94 rows in metadata, but\n",
      "lost 1 that were missing in the data folder.\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 43 potential positive instances and\n",
      "50 potential negative instances. Choosing only\n",
      "40 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "80 volumes range in date from 1785 to 1982.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4, 4, 4, 4, 5, 4, 4, 4, 4, 3]\n",
      "\n",
      "words: 10  reg: 1e-05  acc: 0.6125\n",
      "words: 10  reg: 0.01  acc: 0.625\n",
      "words: 10  reg: 0.1  acc: 0.675\n",
      "words: 10  reg: 1  acc: 0.7\n",
      "True positives 30\n",
      "True negatives 26\n",
      "False positives 14\n",
      "False negatives 10\n",
      "F1 : 0.7142857142857143\n",
      "0.7 0.7\n",
      "246\n",
      "(246, 10)\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 166 370\n",
      "Overlap (assigned to pos class): 0\n",
      "Traintest pos/neg: 41 46\n",
      "Validation to match: 125 neg avail: 324\n",
      "Traintest mean date pos: 1856.8260869565217 neg: 1859.0\n",
      "Validation mean date pos 1886.9626865671642 neg: 1883.208\n",
      "\n",
      "We started with 87 rows in metadata, but\n",
      "lost 1 that were missing in the data folder.\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 40 potential positive instances and\n",
      "46 potential negative instances. Choosing only\n",
      "40 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "80 volumes range in date from 1786 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[5, 5, 4, 5, 4, 4, 3, 4, 3, 3]\n",
      "\n",
      "words: 10  reg: 1e-05  acc: 0.6\n",
      "words: 210  reg: 10  acc: 0.6125\n",
      "words: 310  reg: 10  acc: 0.625\n",
      "True positives 26\n",
      "True negatives 24\n",
      "False positives 16\n",
      "False negatives 14\n",
      "F1 : 0.6341463414634146\n",
      "0.625 0.625\n",
      "259\n",
      "(259, 310)\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 166 370\n",
      "Overlap (assigned to pos class): 0\n",
      "Traintest pos/neg: 42 45\n",
      "Validation to match: 124 neg avail: 323\n",
      "Traintest mean date pos: 1884.0222222222221 neg: 1888.8095238095239\n",
      "Validation mean date pos 1876.2307692307693 neg: 1873.3064516129032\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 42 potential positive instances and\n",
      "45 potential negative instances. Choosing only\n",
      "40 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "80 volumes range in date from 1788 to 1988.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[4, 5, 4, 4, 4, 4, 4, 4, 4, 3]\n",
      "[4, 4, 4, 4, 4, 4, 6, 4, 3, 3]\n",
      "\n",
      "words: 10  reg: 1e-05  acc: 0.6125\n",
      "words: 110  reg: 1e-05  acc: 0.6625\n",
      "words: 310  reg: 0.03  acc: 0.675\n",
      "True positives 26\n",
      "True negatives 28\n",
      "False positives 12\n",
      "False negatives 14\n",
      "F1 : 0.6666666666666667\n",
      "0.675 0.675\n",
      "../reduced_data/hvd.hwpn81.fic.tsv\n",
      "253\n",
      "(253, 310)\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 166 370\n",
      "Overlap (assigned to pos class): 0\n",
      "Traintest pos/neg: 43 48\n",
      "Validation to match: 119 neg avail: 321\n",
      "Traintest mean date pos: 1879.25 neg: 1882.7906976744187\n",
      "Validation mean date pos 1877.488 neg: 1875.7226890756303\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 43 potential positive instances and\n",
      "48 potential negative instances. Choosing only\n",
      "40 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "80 volumes range in date from 1764 to 1989.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[4, 4, 4, 5, 4, 4, 4, 4, 4, 3]\n",
      "[5, 4, 4, 4, 4, 4, 4, 4, 4, 3]\n",
      "\n",
      "words: 10  reg: 1e-05  acc: 0.5375\n",
      "words: 10  reg: 0.01  acc: 0.5625\n",
      "words: 10  reg: 0.1  acc: 0.6375\n",
      "words: 310  reg: 300  acc: 0.65\n",
      "True positives 26\n",
      "True negatives 26\n",
      "False positives 14\n",
      "False negatives 14\n",
      "F1 : 0.65\n",
      "0.65 0.65\n",
      "../reduced_data/hvd.hwpn81.fic.tsv\n",
      "243\n",
      "(243, 310)\n",
      "\n",
      "-------------\n",
      "Begin construction of validation split.\n",
      "Positives/negatives: 166 370\n",
      "Overlap (assigned to pos class): 0\n",
      "Traintest pos/neg: 43 51\n",
      "Validation to match: 123 neg avail: 318\n",
      "Traintest mean date pos: 1860.6666666666667 neg: 1863.4651162790697\n",
      "Validation mean date pos 1885.4883720930231 neg: 1882.040650406504\n",
      "\n",
      "Assigning overlap to positive class.\n",
      "\n",
      "We have 43 potential positive instances and\n",
      "51 potential negative instances. Choosing only\n",
      "40 of each class.\n",
      "MATCHING DATES\n",
      "Instances chosen.\n",
      "\n",
      "80 volumes range in date from 1790 to 1981.\n",
      "\n",
      "Building vocabulary.\n",
      "\n",
      "Authors matched.\n",
      "\n",
      "[4, 4, 5, 4, 4, 4, 4, 4, 4, 3]\n",
      "[4, 4, 5, 4, 4, 3, 3, 7, 3, 3]\n",
      "\n",
      "words: 10  reg: 1e-05  acc: 0.5875\n",
      "words: 10  reg: 0.1  acc: 0.625\n",
      "words: 10  reg: 1  acc: 0.65\n",
      "words: 510  reg: 1  acc: 0.675\n",
      "words: 610  reg: 0.01  acc: 0.7\n",
      "True positives 23\n",
      "True negatives 33\n",
      "False positives 7\n",
      "False negatives 17\n",
      "F1 : 0.6571428571428571\n",
      "0.7 0.7\n",
      "../reduced_data/hvd.hwpn81.fic.tsv\n",
      "251\n",
      "(251, 610)\n"
     ]
    }
   ],
   "source": [
    "train_and_validate('BoWShrunkenGothic', {'lochorror', 'pbgothic', 'locghost', 'stangothic', 'chihorror'},\n",
    "        {'random', 'chirandom'}, 40, '../reduced_data/', '../meta/finalmeta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>floor</th>\n",
       "      <th>ceiling</th>\n",
       "      <th>meandate</th>\n",
       "      <th>testacc</th>\n",
       "      <th>validationacc</th>\n",
       "      <th>features</th>\n",
       "      <th>regularization</th>\n",
       "      <th>i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABsfembeds0</td>\n",
       "      <td>100</td>\n",
       "      <td>1760</td>\n",
       "      <td>1989</td>\n",
       "      <td>1912</td>\n",
       "      <td>0.882653</td>\n",
       "      <td>0.907104</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABsfembeds1</td>\n",
       "      <td>100</td>\n",
       "      <td>1761</td>\n",
       "      <td>1989</td>\n",
       "      <td>1907</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>5000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABsfembeds2</td>\n",
       "      <td>100</td>\n",
       "      <td>1760</td>\n",
       "      <td>1989</td>\n",
       "      <td>1910</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>5500</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABsfembeds3</td>\n",
       "      <td>100</td>\n",
       "      <td>1760</td>\n",
       "      <td>1989</td>\n",
       "      <td>1907</td>\n",
       "      <td>0.922680</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>5500</td>\n",
       "      <td>0.100</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABsfembeds4</td>\n",
       "      <td>100</td>\n",
       "      <td>1761</td>\n",
       "      <td>1989</td>\n",
       "      <td>1920</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.877095</td>\n",
       "      <td>5500</td>\n",
       "      <td>0.003</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name  size  floor  ceiling  meandate   testacc  validationacc  \\\n",
       "0  ABsfembeds0   100   1760     1989      1912  0.882653       0.907104   \n",
       "1  ABsfembeds1   100   1761     1989      1907  0.895000       0.883721   \n",
       "2  ABsfembeds2   100   1760     1989      1910  0.905263       0.911111   \n",
       "3  ABsfembeds3   100   1760     1989      1907  0.922680       0.885714   \n",
       "4  ABsfembeds4   100   1761     1989      1920  0.865000       0.877095   \n",
       "\n",
       "   features  regularization  i  \n",
       "0      5000           0.030  0  \n",
       "1      5000         100.000  1  \n",
       "2      5500           0.010  2  \n",
       "3      5500           0.100  3  \n",
       "4      5500           0.003  4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf = pd.read_csv('../results/ABsfembeds_models.tsv', sep = '\\t')\n",
    "sf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new = sf.loc[[x for x in range(31,41)], : ]\n",
    "old = sf.loc[[x for x in range(21,31)], : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9139999999999999 0.9072845528455284\n"
     ]
    }
   ],
   "source": [
    "print(np.median(new.testacc), np.median(old.testacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8804847501141332 0.8868625750894352\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(new.validationacc), np.mean(old.validationacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>floor</th>\n",
       "      <th>ceiling</th>\n",
       "      <th>meandate</th>\n",
       "      <th>testacc</th>\n",
       "      <th>validationacc</th>\n",
       "      <th>features</th>\n",
       "      <th>regularization</th>\n",
       "      <th>i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ABsfembeds0</td>\n",
       "      <td>125</td>\n",
       "      <td>1771</td>\n",
       "      <td>1989</td>\n",
       "      <td>1931</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>4500</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ABsfembeds1</td>\n",
       "      <td>125</td>\n",
       "      <td>1771</td>\n",
       "      <td>1989</td>\n",
       "      <td>1943</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.875912</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ABsfembeds2</td>\n",
       "      <td>125</td>\n",
       "      <td>1771</td>\n",
       "      <td>1989</td>\n",
       "      <td>1936</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.897260</td>\n",
       "      <td>4000</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ABsfembeds3</td>\n",
       "      <td>125</td>\n",
       "      <td>1818</td>\n",
       "      <td>1989</td>\n",
       "      <td>1940</td>\n",
       "      <td>0.899194</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>5500</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ABsfembeds4</td>\n",
       "      <td>125</td>\n",
       "      <td>1771</td>\n",
       "      <td>1989</td>\n",
       "      <td>1934</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>2500</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ABsfembeds5</td>\n",
       "      <td>125</td>\n",
       "      <td>1818</td>\n",
       "      <td>1989</td>\n",
       "      <td>1939</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.820690</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ABsfembeds6</td>\n",
       "      <td>125</td>\n",
       "      <td>1771</td>\n",
       "      <td>1989</td>\n",
       "      <td>1935</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.899281</td>\n",
       "      <td>5500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>ABsfembeds7</td>\n",
       "      <td>125</td>\n",
       "      <td>1818</td>\n",
       "      <td>1989</td>\n",
       "      <td>1937</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.899281</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.03</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ABsfembeds8</td>\n",
       "      <td>125</td>\n",
       "      <td>1818</td>\n",
       "      <td>1989</td>\n",
       "      <td>1938</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ABsfembeds9</td>\n",
       "      <td>125</td>\n",
       "      <td>1818</td>\n",
       "      <td>1989</td>\n",
       "      <td>1936</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.843972</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.03</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name  size  floor  ceiling  meandate   testacc  validationacc  \\\n",
       "31  ABsfembeds0   125   1771     1989      1931  0.904000       0.876923   \n",
       "32  ABsfembeds1   125   1771     1989      1943  0.916000       0.875912   \n",
       "33  ABsfembeds2   125   1771     1989      1936  0.928000       0.897260   \n",
       "34  ABsfembeds3   125   1818     1989      1940  0.899194       0.923077   \n",
       "35  ABsfembeds4   125   1771     1989      1934  0.919355       0.914286   \n",
       "36  ABsfembeds5   125   1818     1989      1939  0.920000       0.820690   \n",
       "37  ABsfembeds6   125   1771     1989      1935  0.904000       0.899281   \n",
       "38  ABsfembeds7   125   1818     1989      1937  0.904000       0.899281   \n",
       "39  ABsfembeds8   125   1818     1989      1938  0.916000       0.854167   \n",
       "40  ABsfembeds9   125   1818     1989      1936  0.912000       0.843972   \n",
       "\n",
       "    features  regularization  i  \n",
       "31      4500            0.03  0  \n",
       "32      4000            0.10  1  \n",
       "33      4000           10.00  2  \n",
       "34      5500            0.03  3  \n",
       "35      2500            1.00  4  \n",
       "36      6000            0.03  5  \n",
       "37      5500            0.01  6  \n",
       "38      5000            0.03  7  \n",
       "39      5000            0.01  8  \n",
       "40      1000            0.03  9  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>floor</th>\n",
       "      <th>ceiling</th>\n",
       "      <th>meandate</th>\n",
       "      <th>testacc</th>\n",
       "      <th>validationacc</th>\n",
       "      <th>features</th>\n",
       "      <th>regularization</th>\n",
       "      <th>i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ABsfembeds0</td>\n",
       "      <td>125</td>\n",
       "      <td>1771</td>\n",
       "      <td>1989</td>\n",
       "      <td>1944</td>\n",
       "      <td>0.922764</td>\n",
       "      <td>0.888158</td>\n",
       "      <td>4500</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ABsfembeds1</td>\n",
       "      <td>125</td>\n",
       "      <td>1771</td>\n",
       "      <td>1989</td>\n",
       "      <td>1943</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>5000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ABsfembeds2</td>\n",
       "      <td>125</td>\n",
       "      <td>1771</td>\n",
       "      <td>1989</td>\n",
       "      <td>1942</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.871429</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ABsfembeds3</td>\n",
       "      <td>125</td>\n",
       "      <td>1771</td>\n",
       "      <td>1989</td>\n",
       "      <td>1940</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ABsfembeds4</td>\n",
       "      <td>125</td>\n",
       "      <td>1818</td>\n",
       "      <td>1989</td>\n",
       "      <td>1937</td>\n",
       "      <td>0.911290</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>4500</td>\n",
       "      <td>0.010</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ABsfembeds5</td>\n",
       "      <td>125</td>\n",
       "      <td>1771</td>\n",
       "      <td>1989</td>\n",
       "      <td>1943</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ABsfembeds6</td>\n",
       "      <td>125</td>\n",
       "      <td>1836</td>\n",
       "      <td>1989</td>\n",
       "      <td>1938</td>\n",
       "      <td>0.923387</td>\n",
       "      <td>0.875862</td>\n",
       "      <td>3500</td>\n",
       "      <td>0.010</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ABsfembeds7</td>\n",
       "      <td>125</td>\n",
       "      <td>1771</td>\n",
       "      <td>1989</td>\n",
       "      <td>1939</td>\n",
       "      <td>0.891129</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>4500</td>\n",
       "      <td>10.000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ABsfembeds8</td>\n",
       "      <td>125</td>\n",
       "      <td>1771</td>\n",
       "      <td>1989</td>\n",
       "      <td>1942</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.881119</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ABsfembeds9</td>\n",
       "      <td>125</td>\n",
       "      <td>1771</td>\n",
       "      <td>1989</td>\n",
       "      <td>1939</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>3500</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name  size  floor  ceiling  meandate   testacc  validationacc  \\\n",
       "21  ABsfembeds0   125   1771     1989      1944  0.922764       0.888158   \n",
       "22  ABsfembeds1   125   1771     1989      1943  0.910569       0.892857   \n",
       "23  ABsfembeds2   125   1771     1989      1942  0.910569       0.871429   \n",
       "24  ABsfembeds3   125   1771     1989      1940  0.900000       0.896552   \n",
       "25  ABsfembeds4   125   1818     1989      1937  0.911290       0.875000   \n",
       "26  ABsfembeds5   125   1771     1989      1943  0.902439       0.909091   \n",
       "27  ABsfembeds6   125   1836     1989      1938  0.923387       0.875862   \n",
       "28  ABsfembeds7   125   1771     1989      1939  0.891129       0.891892   \n",
       "29  ABsfembeds8   125   1771     1989      1942  0.904000       0.881119   \n",
       "30  ABsfembeds9   125   1771     1989      1939  0.890244       0.886667   \n",
       "\n",
       "    features  regularization  i  \n",
       "21      4500          10.000  0  \n",
       "22      5000         100.000  1  \n",
       "23      2000           0.010  2  \n",
       "24      3000           0.030  3  \n",
       "25      4500           0.010  4  \n",
       "26      4000           0.003  5  \n",
       "27      3500           0.010  6  \n",
       "28      4500          10.000  7  \n",
       "29      4000           0.010  8  \n",
       "30      3500           0.010  9  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300.0 3850.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(new.features), np.mean(old.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist = pd.read_csv('../results/HistShrunkenGothic_models.tsv', sep = '\\t')\n",
    "hist1990 = pd.read_csv('../results/Hist1990ShrunkenGothic_models.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow = pd.read_csv('../results/BoWShrunkenGothic_models.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove = pd.read_csv('../results/GloveShrunkenGothic_models.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7008620689655172 0.6737499999999998 0.6952499999999999\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(hist.testacc), np.mean(bow.testacc), np.mean(glove.testacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6332773010353246 0.6586671889400366 0.6041799522115593 0.6269215831334122\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(hist.validationacc), np.mean(hist1990.validationacc), np.mean(bow.validationacc), np.mean(glove.validationacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425.86206896551727 615.0 456.6666666666667 262.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(hist.features), np.mean(hist1990.features), np.mean(bow.features), np.mean(glove.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9228000000000002 0.924\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(myst.testacc[0:10]), np.mean(myst.testacc[10: ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist = pd.read_csv('../results/HistGothic_models.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7946605264232719 0.7913934965662057\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(hist.validationacc), np.mean(bowgoth.validationacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7946605264232719 0.7925821663697067 0.7913934965662057\n"
     ]
    }
   ],
   "source": [
    "hist = pd.read_csv('../results/HistGothic_models.tsv', sep = '\\t')\n",
    "hist1990 = pd.read_csv('../results/Hist1990Gothic_models.tsv', sep = '\\t')\n",
    "bow = pd.read_csv('../results/BoWGothic_models.tsv', sep = '\\t')\n",
    "print(np.mean(hist.validationacc), np.mean(hist1990.validationacc), np.mean(bow.validationacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow = pd.read_csv('BoWMystery_models.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9281187139024163"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(bow.validationacc[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9209654471544717"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(bow.validationacc[30: ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
