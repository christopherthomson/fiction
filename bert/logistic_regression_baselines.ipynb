{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic models for blog post\n",
    "\n",
    "This notebook works up some quick and dirty bag-of-words models, to see how much this approach suffers when we cut whole documents into 128- or 256-word chunks.\n",
    "\n",
    "We're going to use LogisticRegression from scikit-learn, and apply it in three ways:\n",
    "\n",
    "1. To whole documents.\n",
    "\n",
    "2. To BERT-sized chunks.\n",
    "\n",
    "3. Aggregating the votes from BERT-sized chunks to produce a document-level prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things that will come in handy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import Counter\n",
    "from scipy.stats import pearsonr\n",
    "import random, glob, csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling whole movie reviews from the IMDb dataset\n",
    "\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('sentimentdata.tsv', sep = '\\t')\n",
    "\n",
    "fullname = 'sentiment'\n",
    "\n",
    "raw = raw.sample(frac = 1)\n",
    "# that is in effect a shuffle\n",
    "\n",
    "cut = round(len(raw) * .75)\n",
    "\n",
    "train = raw.iloc[0: cut, : ]\n",
    "test = raw.iloc[cut : , : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = Counter()\n",
    "\n",
    "delchars = ''.join(c for c in map(chr, range(256)) if not c.isalpha())\n",
    "spaces = ' ' * len(delchars)\n",
    "punct2space = str.maketrans(delchars, spaces)\n",
    "\n",
    "def getwords(text):\n",
    "    global punct2space\n",
    "    text = text.replace('<br />', ' ')\n",
    "    words = text.translate(punct2space).split()\n",
    "    return words\n",
    "\n",
    "def get_dataset(rootfolder):\n",
    "    \n",
    "    negpaths = glob.glob(rootfolder + '/neg/*.txt')\n",
    "    pospaths = glob.glob(rootfolder + '/pos/*.txt')\n",
    "    paths = [(0, x) for x in negpaths] + [(1, x) for x in pospaths]\n",
    "    \n",
    "    index = 0\n",
    "    lines = []\n",
    "    lex = Counter()\n",
    "    labels = []\n",
    "    texts = []\n",
    "    \n",
    "    for label, p in paths:\n",
    "        \n",
    "        with open(p) as f:\n",
    "            text = f.read().strip().lower()\n",
    "            words = getwords(text)\n",
    "            for w in words:\n",
    "                lex[w] += 1\n",
    "            labels.append(label)\n",
    "            texts.append(text)\n",
    "\n",
    "    vocab = [x[0] for x in lex.most_common()]\n",
    "    print(vocab[0:10])\n",
    "    \n",
    "    df = pd.DataFrame.from_dict({'sent': labels, 'text': texts})\n",
    "    df = df.sample(frac = 1)\n",
    "    # shuffle\n",
    "    \n",
    "    return vocab, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrix(df, vocab, cut):\n",
    "    \n",
    "    lexicon = dict()\n",
    "    for i in range(cut):\n",
    "        lexicon[vocab[i]] = i\n",
    "    \n",
    "    y = []\n",
    "    x = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        y.append(int(row['sent']))\n",
    "        x_row = np.zeros(cut)\n",
    "        words = getwords(row.text)\n",
    "        for w in words:\n",
    "            if w in lexicon:\n",
    "                idx = lexicon[w]\n",
    "                x_row[idx] = x_row[idx] + 1\n",
    "        \n",
    "        x_row = x_row / np.sum(len(words))\n",
    "        \n",
    "        x.append(x_row)\n",
    "    \n",
    "    x = np.array(x)\n",
    "    \n",
    "    return x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'and', 'a', 'of', 'to', 'is', 'it', 'in', 'i', 'this']\n",
      "got training\n",
      "['the', 'and', 'a', 'of', 'to', 'is', 'it', 'in', 'i', 'this']\n",
      "got test\n",
      "3200 1e-05 0.8492538504508818 0.84888\n",
      "3200 0.0001 0.8725717483411942 0.87248\n",
      "3200 0.0003 0.8759947214779862 0.87596\n",
      "3200 0.001 0.8708091908091908 0.87068\n",
      "3200 0.01 0.8593687574910108 0.8592\n",
      "3200 0.1 0.8512644321041908 0.85108\n",
      "3400 1e-05 0.8499640488935049 0.84976\n",
      "3400 0.0001 0.8742203742203742 0.87416\n",
      "3400 0.0003 0.8748 0.8748\n",
      "3400 0.001 0.8709380870538391 0.87084\n",
      "3400 0.01 0.8584463921239045 0.85852\n",
      "3400 0.1 0.8480916336256958 0.84828\n",
      "3600 1e-05 0.8521301254895692 0.852\n",
      "3600 0.0001 0.8736008954269269 0.87352\n",
      "3600 0.0003 0.8757154865308411 0.8758\n",
      "3600 0.001 0.8704771053474224 0.87056\n",
      "3600 0.01 0.8545301407772832 0.85492\n",
      "3600 0.1 0.8419151583256411 0.84244\n",
      "3800 1e-05 0.8524065027628734 0.85256\n",
      "3800 0.0001 0.8751649934002641 0.87516\n",
      "3800 0.0003 0.8758065162505511 0.87604\n",
      "3800 0.001 0.8714886796233221 0.87172\n",
      "3800 0.01 0.8534115566985454 0.85408\n",
      "3800 0.1 0.8392627173213135 0.84024\n",
      "4000 1e-05 0.8529435322386865 0.85312\n",
      "4000 0.0001 0.8755904251060764 0.87568\n",
      "4000 0.0003 0.8764180061730871 0.87668\n",
      "4000 0.001 0.8711863047748868 0.87148\n",
      "4000 0.01 0.8523853468683545 0.85284\n",
      "4000 0.1 0.8385646624387895 0.83912\n",
      "4200 1e-05 0.853651696646769 0.85388\n",
      "4200 0.0001 0.8747645573678516 0.875\n",
      "4200 0.0003 0.875270888514327 0.87568\n",
      "4200 0.001 0.8710971988121036 0.87152\n",
      "4200 0.01 0.8497605344709622 0.85068\n",
      "4200 0.1 0.8349569277835923 0.836\n",
      "4400 1e-05 0.8540999359385011 0.85424\n",
      "4400 0.0001 0.8760264370118166 0.8762\n",
      "4400 0.0003 0.8754516983859311 0.87592\n",
      "4400 0.001 0.8709379393452501 0.87148\n",
      "4400 0.01 0.8495090938355062 0.8504\n",
      "4400 0.1 0.8326770071526158 0.83344\n",
      "4600 1e-05 0.854051454676605 0.85432\n",
      "4600 0.0001 0.8750802310654686 0.87544\n",
      "4600 0.0003 0.8748443087307645 0.8754\n",
      "4600 0.001 0.8700233250221184 0.87072\n",
      "4600 0.01 0.8499718445820931 0.8508\n",
      "4600 0.1 0.8285047481088041 0.82952\n",
      "4800 1e-05 0.8537866335244356 0.85412\n",
      "4800 0.0001 0.8751154108626712 0.87556\n",
      "4800 0.0003 0.8741705875256364 0.87484\n",
      "4800 0.001 0.8679169349645848 0.86872\n",
      "4800 0.01 0.847539596179422 0.84868\n",
      "4800 0.1 0.824866040852504 0.82612\n",
      "5000 1e-05 0.8540813869491933 0.85456\n",
      "5000 0.0001 0.8754522794886227 0.87608\n",
      "5000 0.0003 0.8742953776775648 0.87512\n",
      "5000 0.001 0.8682489706950837 0.86944\n",
      "5000 0.01 0.8438511326860841 0.8456\n",
      "5000 0.1 0.8213622040882281 0.82312\n",
      "(0.87668, 4000, 0.0003)\n"
     ]
    }
   ],
   "source": [
    "triplets = []\n",
    "\n",
    "vocab, train_df = get_dataset('/Volumes/TARDIS/aclImdb/train')\n",
    "print('got training')\n",
    "dummy, test_df = get_dataset('/Volumes/TARDIS/aclImdb/test')\n",
    "print('got test')\n",
    "\n",
    "for cut in range(3200, 5200, 200):\n",
    "\n",
    "    for reg_const in [.00001, .0001, .0003, .001, .01, .1]:\n",
    "        \n",
    "        trainingset, train_y = make_matrix(train_df, vocab, cut)\n",
    "        testset, test_y = make_matrix(test_df, vocab, cut)\n",
    "        \n",
    "        model = LogisticRegression(C = reg_const)\n",
    "        stdscaler = StandardScaler()\n",
    "        stdscaler.fit(trainingset)\n",
    "        scaledtraining = stdscaler.transform(trainingset)\n",
    "        model.fit(scaledtraining, train_y)\n",
    "\n",
    "        scaledtest = stdscaler.transform(testset)\n",
    "        predictions = [x[1] for x in model.predict_proba(scaledtest)]\n",
    "        predictions = np.round(predictions)\n",
    "        accuracy = accuracy_score(predictions, test_y)\n",
    "        f1 = f1_score(predictions, test_y)\n",
    "        print(cut, reg_const, f1, accuracy)\n",
    "        triplets.append((accuracy, cut, reg_const))\n",
    "\n",
    "random.shuffle(triplets)\n",
    "triplets.sort(key = lambda x: x[0])\n",
    "print(triplets[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cut down the reviews to 128-word chunks; how does it perform?\n",
    "\n",
    "Here I'm using the same data files that were given to BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i', 'this']\n",
      "got training\n",
      "['the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i', 'this']\n",
      "got test\n",
      "2200 1e-05 0.7959050511868602 0.792454125291722\n",
      "2200 5e-05 0.8080544755826236 0.8045806067816775\n",
      "2200 0.0001 0.8107440429402385 0.8071889443097058\n",
      "2200 0.0003 0.8112095722189735 0.8079439893836087\n",
      "2200 0.001 0.8087246808318672 0.8059762961607102\n",
      "2600 1e-05 0.7992070107459056 0.796069189584954\n",
      "2600 5e-05 0.8111916445324809 0.8080812703061364\n",
      "2600 0.0001 0.8132145107570438 0.8100947238365441\n",
      "2600 0.0003 0.8128761750185974 0.8100718436827895\n",
      "2600 0.001 0.8105947703563201 0.8080812703061364\n",
      "3000 1e-05 0.8009198719393966 0.7979682423465886\n",
      "3000 5e-05 0.8134257800189095 0.8103692856815998\n",
      "3000 0.0001 0.8152501914155745 0.8122912185969889\n",
      "3000 0.0003 0.8147196682891653 0.8118793758294056\n",
      "3000 0.001 0.8115392429064807 0.8089735963025672\n",
      "3400 1e-05 0.8019611830362187 0.7994554523406397\n",
      "3400 5e-05 0.813970803907854 0.8113531322930491\n",
      "3400 0.0001 0.8166403607666292 0.8139385896673226\n",
      "3400 0.0003 0.8157609676836974 0.8132064247471743\n",
      "3400 0.001 0.8125777064448314 0.8103006452203358\n",
      "3800 1e-05 0.8046912073757712 0.802246831098705\n",
      "3800 5e-05 0.8157966040462428 0.8133437056697022\n",
      "3800 0.0001 0.8172901305553676 0.814602114126207\n",
      "3800 0.0003 0.81690077232284 0.8144877133574338\n",
      "3800 0.001 0.8115555153379784 0.8094083192239052\n",
      "4200 1e-05 0.8045966610867303 0.8023612318674781\n",
      "4200 5e-05 0.8160202513335142 0.8137555484372855\n",
      "4200 0.0001 0.8173111673292375 0.8149453164325264\n",
      "4200 0.0003 0.815909604519774 0.8136182675147577\n",
      "4200 0.001 0.8102952171647372 0.808424472612456\n",
      "4600 1e-05 0.8044088357776571 0.8022697112524596\n",
      "4600 5e-05 0.8165902365193034 0.8142360316661328\n",
      "4600 0.0001 0.8185410128468538 0.8161122042740128\n",
      "4600 0.0003 0.8158043443864288 0.8135496270534938\n",
      "4600 0.001 0.8097373188405798 0.8077609481535716\n",
      "5000 1e-05 0.8057524629147322 0.8037569212465108\n",
      "5000 5e-05 0.8170797901212231 0.8149453164325264\n",
      "5000 0.0001 0.8180934951083396 0.8157918821214478\n",
      "5000 0.0003 0.815861584799692 0.8139614698210772\n",
      "5000 0.001 0.8096383355266142 0.8080355099986272\n",
      "5400 1e-05 0.8065817506006073 0.80474076785796\n",
      "5400 5e-05 0.8178139482310387 0.8156088408914108\n",
      "5400 0.0001 0.8183954802259887 0.8161350844277674\n",
      "5400 0.0003 0.8145349231848316 0.8124513796732714\n",
      "5400 0.001 0.8082092262985833 0.8067084610808585\n",
      "5800 1e-05 0.807816112798658 0.8060220564682194\n",
      "5800 5e-05 0.8179370573994886 0.8158834027364664\n",
      "5800 0.0001 0.8182928484190528 0.8161808447352766\n",
      "5800 0.0003 0.8143604137899813 0.8123598590582529\n",
      "5800 0.001 0.8063528077141235 0.8047178877042054\n",
      "(0.8161808447352766, 5800, 0.0001)\n"
     ]
    }
   ],
   "source": [
    "def get_datachunks(filepath):\n",
    "    \n",
    "    data = pd.read_csv(filepath, sep = '\\t', header = None, names = ['idx', 'sent', 'dummy', 'text'], quoting = csv.QUOTE_NONE)\n",
    "    \n",
    "    lex = Counter()\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        text = row['text'].strip().lower()\n",
    "        words = getwords(text)\n",
    "        for w in words:\n",
    "            lex[w] += 1\n",
    "\n",
    "    vocab = [x[0] for x in lex.most_common()]\n",
    "    print(vocab[0:10])\n",
    "    \n",
    "    df = data.loc[ : , ['sent', 'text']]\n",
    "    \n",
    "    return vocab, df\n",
    "\n",
    "triplets = []\n",
    "\n",
    "vocab, train_df = get_datachunks('/Users/tunder/Dropbox/fiction/bert/bertdata/train_sentiment.tsv')\n",
    "print('got training')\n",
    "dummy, test_df = get_datachunks('/Users/tunder/Dropbox/fiction/bert/bertdata/dev_sentiment.tsv')\n",
    "print('got test')\n",
    "\n",
    "for cut in range(2200, 6200, 400):\n",
    "\n",
    "    for reg_const in [.00001, .00005, .0001, .0003, .001]:\n",
    "        \n",
    "        trainingset, train_y = make_matrix(train_df, vocab, cut)\n",
    "        testset, test_y = make_matrix(test_df, vocab, cut)\n",
    "        \n",
    "        model = LogisticRegression(C = reg_const)\n",
    "        stdscaler = StandardScaler()\n",
    "        stdscaler.fit(trainingset)\n",
    "        scaledtraining = stdscaler.transform(trainingset)\n",
    "        model.fit(scaledtraining, train_y)\n",
    "\n",
    "        scaledtest = stdscaler.transform(testset)\n",
    "        predictions = [x[1] for x in model.predict_proba(scaledtest)]\n",
    "        predictions = np.round(predictions)\n",
    "        accuracy = accuracy_score(predictions, test_y)\n",
    "        f1 = f1_score(predictions, test_y)\n",
    "        print(cut, reg_const, f1, accuracy)\n",
    "        triplets.append((accuracy, cut, reg_const))\n",
    "\n",
    "random.shuffle(triplets)\n",
    "triplets.sort(key = lambda x: x[0])\n",
    "print(triplets[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How much can we improve our chunk-level results by aggregating them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset, train_y = make_matrix(train_df, vocab, 5200)\n",
    "testset, test_y = make_matrix(test_df, vocab, 5200)\n",
    "model = LogisticRegression(C = .0001)\n",
    "stdscaler = StandardScaler()\n",
    "stdscaler.fit(trainingset)\n",
    "scaledtraining = stdscaler.transform(trainingset)\n",
    "model.fit(scaledtraining, train_y)\n",
    "\n",
    "scaledtest = stdscaler.transform(testset)\n",
    "predictions = [x[1] for x in model.predict_proba(scaledtest)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.161673</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38639</th>\n",
       "      <td>0.284180</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5648</th>\n",
       "      <td>0.055561</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43581</th>\n",
       "      <td>0.590964</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36158</th>\n",
       "      <td>0.660262</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pred  real\n",
       "idx                  \n",
       "996    0.161673     0\n",
       "38639  0.284180     1\n",
       "5648   0.055561     0\n",
       "43581  0.590964     1\n",
       "36158  0.660262     1"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a dataframe\n",
    "meta = pd.read_csv('bertmeta/dev_rows_sentiment.tsv', sep = '\\t')\n",
    "pred = pd.DataFrame.from_dict({'idx': meta['idx'], 'pred': predictions, 'real': test_y})\n",
    "pred = pred.set_index('idx')\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8162266050427859\n"
     ]
    }
   ],
   "source": [
    "right = 0\n",
    "\n",
    "for idx, row in pred.iterrows():\n",
    "    if row['pred'] >= 0.5:\n",
    "        predclass = 1\n",
    "    else:\n",
    "        predclass = 0\n",
    "        \n",
    "    if predclass == row['real']:\n",
    "        right += 1\n",
    "\n",
    "print(right / len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall accuracy: 0.86454402849027\n"
     ]
    }
   ],
   "source": [
    "byvol = meta.groupby('docid')\n",
    "rightvols = 0\n",
    "allvols = 0\n",
    "bertprobs = dict()\n",
    "\n",
    "for vol, df in byvol:\n",
    "    total = 0\n",
    "    right = 0\n",
    "    positive = 0\n",
    "    df.set_index('idx', inplace = True)\n",
    "    predicted = []\n",
    "    for idx, row in df.iterrows():\n",
    "        predict = pred.loc[idx, 'pred']\n",
    "        predicted.append(predict)\n",
    "        true_class = row['class']\n",
    "    \n",
    "    volmean = sum(predicted) / len(predicted)\n",
    "    if volmean >= 0.5:\n",
    "        predicted_class = 1\n",
    "    else:\n",
    "        predicted_class = 0\n",
    "    \n",
    "    if true_class == predicted_class:\n",
    "        rightvols += 1\n",
    "    allvols += 1\n",
    "\n",
    "print()\n",
    "print('Overall accuracy:', rightvols / allvols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about the parallel problem for genre?\n",
    "\n",
    "We use the same data that was passed to BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'and', 'to', 'a', 'of', 'i', 'he', 'in', 'was', 'it']\n",
      "got training\n",
      "['the', 'and', 'to', 'of', 'a', 'i', 'he', 'in', 'was', '”']\n",
      "got test\n",
      "2000 1e-05 0.7380035630136199 0.744955805019953\n",
      "2000 5e-05 0.7374129774222086 0.7453847014507888\n",
      "2000 0.0001 0.7321996032586715 0.7407041360534069\n",
      "2000 0.0003 0.7232408229436059 0.7325923992093387\n",
      "2000 0.001 0.7135843513220967 0.7236788125163167\n",
      "2400 1e-05 0.7414472798600026 0.7492820646701227\n",
      "2400 5e-05 0.7369196471842591 0.7469324581359789\n",
      "2400 0.0001 0.73058597016667 0.7416738149405139\n",
      "2400 0.0003 0.7198266606804739 0.732349979487562\n",
      "2400 0.001 0.7091865673101371 0.7227277813001156\n",
      "2800 1e-05 0.7440613026819923 0.7508671166971246\n",
      "2800 5e-05 0.7382283753036907 0.7468392197814493\n",
      "2800 0.0001 0.7325671089170009 0.7413940998769254\n",
      "2800 0.0003 0.7211278485901893 0.7307276321187484\n",
      "2800 0.001 0.7093805583481555 0.7196882109424533\n",
      "3200 1e-05 0.7449090804795502 0.7512214224443367\n",
      "3200 5e-05 0.7397687260693052 0.7477902509976504\n",
      "3200 0.0001 0.7333962009969016 0.7416738149405139\n",
      "3200 0.0003 0.7207193330381039 0.7295155335098646\n",
      "3200 0.001 0.7086511054618287 0.7178980345354865\n",
      "3600 1e-05 0.7465601589970954 0.7526945884459031\n",
      "3600 5e-05 0.743108178103478 0.7502890388990415\n",
      "3600 0.0001 0.7347056449287896 0.7426061984858091\n",
      "3600 0.0003 0.7219459750033598 0.7299257822697945\n",
      "3600 0.001 0.7090664617748752 0.7175623764591802\n",
      "4000 1e-05 0.7449929154061196 0.7516503188751725\n",
      "4000 5e-05 0.7397850289384121 0.7476410696304031\n",
      "4000 0.0001 0.7330264450393883 0.7415246335732667\n",
      "4000 0.0003 0.7172339442476513 0.7261030097340843\n",
      "4000 0.001 0.7014902383611229 0.7108865102748667\n",
      "4400 1e-05 0.745502665797246 0.7525454070786559\n",
      "4400 5e-05 0.7386530014641287 0.7470256964905083\n",
      "4400 0.0001 0.7299945983486381 0.7390071980009697\n",
      "4400 0.0003 0.7142279163857047 0.7233990974527281\n",
      "4400 0.001 0.6978890298765716 0.7069704993846269\n",
      "4800 1e-05 0.7435098650051922 0.7512773654570544\n",
      "4800 5e-05 0.7361022364217251 0.7458508932234363\n",
      "4800 0.0001 0.7272339021217945 0.737981576101145\n",
      "4800 0.0003 0.7111723495507385 0.722466713907433\n",
      "4800 0.001 0.6922032056506383 0.7042106440905531\n",
      "5200 1e-05 0.7429693364901542 0.7509976503934659\n",
      "5200 5e-05 0.7343204899604621 0.74437772722187\n",
      "5200 0.0001 0.7265977297465401 0.7377018610375564\n",
      "5200 0.0003 0.7106312615504328 0.7226158952746802\n",
      "5200 0.001 0.6916382750900417 0.7046395405213889\n",
      "5600 1e-05 0.7468342268595914 0.7543169358147167\n",
      "5600 5e-05 0.7391068802724248 0.7485548055047925\n",
      "5600 0.0001 0.7304648094979437 0.7409092604333719\n",
      "5600 0.0003 0.7129800322123687 0.7241822996307761\n",
      "5600 0.001 0.6942886529915194 0.7062432402192966\n",
      "6000 1e-05 0.747579157700584 0.7550068996382352\n",
      "6000 5e-05 0.7392405063291139 0.7483869764666393\n",
      "6000 0.0001 0.7299044080653276 0.7397158094953941\n",
      "6000 0.0003 0.7117383980949062 0.7223548278819976\n",
      "6000 0.001 0.6930394431554525 0.7039495766978704\n",
      "(0.7550068996382352, 6000, 1e-05)\n"
     ]
    }
   ],
   "source": [
    "triplets = []\n",
    "\n",
    "vocab, train_df = get_datachunks('/Users/tunder/Dropbox/fiction/bert/bertdata/train_Mystery256.tsv')\n",
    "print('got training')\n",
    "dummy, test_df = get_datachunks('/Users/tunder/Dropbox/fiction/bert/bertdata/dev_Mystery256.tsv')\n",
    "print('got test')\n",
    "\n",
    "for cut in range(2000, 6200, 400):\n",
    "\n",
    "    for reg_const in [.00001, .00005, .0001, .0003, .001]:\n",
    "        \n",
    "        trainingset, train_y = make_matrix(train_df, vocab, cut)\n",
    "        testset, test_y = make_matrix(test_df, vocab, cut)\n",
    "        \n",
    "        model = LogisticRegression(C = reg_const)\n",
    "        stdscaler = StandardScaler()\n",
    "        stdscaler.fit(trainingset)\n",
    "        scaledtraining = stdscaler.transform(trainingset)\n",
    "        model.fit(scaledtraining, train_y)\n",
    "\n",
    "        scaledtest = stdscaler.transform(testset)\n",
    "        predictions = [x[1] for x in model.predict_proba(scaledtest)]\n",
    "        predictions = np.round(predictions)\n",
    "        accuracy = accuracy_score(predictions, test_y)\n",
    "        f1 = f1_score(predictions, test_y)\n",
    "        print(cut, reg_const, f1, accuracy)\n",
    "        triplets.append((accuracy, cut, reg_const))\n",
    "\n",
    "random.shuffle(triplets)\n",
    "triplets.sort(key = lambda x: x[0])\n",
    "print(triplets[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### and now aggregating the genre chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42274</th>\n",
       "      <td>0.372493</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47664</th>\n",
       "      <td>0.248213</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>0.545889</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17125</th>\n",
       "      <td>0.713855</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33412</th>\n",
       "      <td>0.247855</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pred  real\n",
       "idx                  \n",
       "42274  0.372493     0\n",
       "47664  0.248213     0\n",
       "834    0.545889     1\n",
       "17125  0.713855     1\n",
       "33412  0.247855     0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best model\n",
    "\n",
    "trainingset, train_y = make_matrix(train_df, vocab, 6000)\n",
    "testset, test_y = make_matrix(test_df, vocab, 6000)\n",
    "model = LogisticRegression(C = .00001)\n",
    "stdscaler = StandardScaler()\n",
    "stdscaler.fit(trainingset)\n",
    "scaledtraining = stdscaler.transform(trainingset)\n",
    "model.fit(scaledtraining, train_y)\n",
    "\n",
    "scaledtest = stdscaler.transform(testset)\n",
    "predictions = [x[1] for x in model.predict_proba(scaledtest)]\n",
    "\n",
    "# make a dataframe\n",
    "meta = pd.read_csv('bertmeta/dev_rows_Mystery256.tsv', sep = '\\t')\n",
    "pred = pd.DataFrame.from_dict({'idx': meta['idx'], 'pred': predictions, 'real': test_y})\n",
    "pred = pred.set_index('idx')\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall accuracy: 0.8770491803278688\n"
     ]
    }
   ],
   "source": [
    "byvol = meta.groupby('docid')\n",
    "rightvols = 0\n",
    "allvols = 0\n",
    "bertprobs = dict()\n",
    "\n",
    "for vol, df in byvol:\n",
    "    total = 0\n",
    "    right = 0\n",
    "    positive = 0\n",
    "    df.set_index('idx', inplace = True)\n",
    "    predicted = []\n",
    "    for idx, row in df.iterrows():\n",
    "        predict = pred.loc[idx, 'pred']\n",
    "        predicted.append(predict)\n",
    "        true_class = row['class']\n",
    "    \n",
    "    volmean = sum(predicted) / len(predicted)\n",
    "    if volmean >= 0.5:\n",
    "        predicted_class = 1\n",
    "    else:\n",
    "        predicted_class = 0\n",
    "    \n",
    "    if true_class == predicted_class:\n",
    "        rightvols += 1\n",
    "    allvols += 1\n",
    "\n",
    "print()\n",
    "print('Overall accuracy:', rightvols / allvols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside:** It's really remarkable how powerful binary voting can be. In this case models of genre at 256-word scale are pretty awful (75.5% accuracy) but aggregate up to 87.7% accuracy. But that's still not quite in the same league with models that can see whole novels; in that case the detective/mystery genre can be modeled with more than 91% accuracy. Something is lost when we can't see the whole elephant at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
